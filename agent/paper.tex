% ==============================================================================
% AI Agents4QualResearch 2026 - FINAL SUBMISSION
% ==============================================================================
% AI Agent Lead Author: Claude 3.5 Sonnet (Anthropic)
% Title: Understanding Error Patterns in Neuro-Symbolic Legal Reasoning:
%        An AI Agent's Investigation of Mixed-Performance Cases
%
% CONFERENCE FORMAT REQUIREMENTS (from official Word template):
% - Font: Times New Roman, 12pt, 1.5 line spacing
% - Title: 18pt, centered
% - Headers: H1=16pt, H2=14pt, H3=12pt (all bold, lowercase except first word)
% - Abstract: max 200 words
% - Main body: max 3500 words (excluding references and author reflection)
% - Author Reflection: max 1000 words (MANDATORY)
% - AI Involvement Checklist: MANDATORY (papers without = desk rejected)
% - Citations: APA 7th edition
% - Margins: Standard A4
%
% STRUCTURE:
% 1. Title and Abstract
% 2. Introduction (research questions, AI agent role)
% 3. Methods (data sources, analysis approach, epistemic limitations)
% 4. Results (error patterns, confusion matrices, formula analysis)
% 5. Discussion (why symbolic reasoning fails, reproducibility, AI research)
% 6. Conclusion
% 7. References
% 8. Author Reflection (prompting, surprises, failures, human relationship)
% 9. AI Involvement Checklist (scoring 1-4 for each research component)
%
% SUBMISSION DETAILS:
% - Deadline: January 31, 2026
% - Platform: OpenReview (anonymous submission)
% - Review: 3 LLMs independently evaluate, then human organizers review top papers
% - Conference: March 13, 2026 (virtual, free)
%
% ==============================================================================

\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry} % Standard A4 margins
\usepackage{times} % Times New Roman
\usepackage{setspace} % For 1.5 spacing
\usepackage{titlesec} % Section formatting
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{booktabs} % Professional tables
\usepackage{caption}
\usepackage{enumitem} % Control list spacing

% Configure list spacing to match document
\setlist{nosep, leftmargin=*, before={\parskip=6pt}, after={\parskip=6pt}}

% Font and spacing
\setstretch{1.5} % 1.5 line spacing throughout
\setlength{\parskip}{6pt} % Spacing before paragraphs: 6pt

% Section formatting (numbering OFF)
\setcounter{secnumdepth}{0}

% Header 1: 16pt, bold, spacing before 24
\titleformat{\section}
  {\normalfont\fontsize{16}{19.2}\bfseries\raggedright}
  {\thesection}{0em}{}
\titlespacing*{\section}{0pt}{24pt}{6pt}

% Header 2: 14pt, bold, spacing before 10
\titleformat{\subsection}
  {\normalfont\fontsize{14}{16.8}\bfseries\raggedright}
  {\thesubsection}{0em}{}
\titlespacing*{\subsection}{0pt}{10pt}{6pt}

% Header 3: 12pt, bold, spacing before 10
\titleformat{\subsubsection}
  {\normalfont\fontsize{12}{14.4}\bfseries\raggedright}
  {\thesubsubsection}{0em}{}
\titlespacing*{\subsubsection}{0pt}{10pt}{6pt}

\begin{document}

% ==============================================================================
% TITLE (18pt, centered, bold per template)
% ==============================================================================
\begin{center}
{\fontsize{18}{21.6}\selectfont\bfseries
Understanding Error Patterns in Neuro-Symbolic Legal Reasoning: An AI Agent's Investigation of Mixed-Performance Cases
}
\end{center}

\vspace{12pt}

% ==============================================================================
% ABSTRACT (max 200 words)
% Purpose: Summarize research question, methods, key findings, and contribution
% ==============================================================================
\section{Abstract}

This paper documents an AI agent-led investigation into error patterns of Logify, an ongoing neuro-symbolic reasoning system for legal document analysis. The system aims to improve upon retrieval-augmented generation (RAG) baselines, but shows mixed performance: while achieving comparable results on some ContractNLI documents, it underperforms on others (40\% vs 68\% on a 7-document subset). We identify three systematic error patterns in underperforming cases: (1) complete inability to detect contradictions (100\% error rate on FALSE cases), (2) over-confident predictions on uncertain evidence (75\% false positive rate), and (3) lossy query translation that strips semantic negations. This work exemplifies AI-driven qualitative research methodology, where an LLM agent conducts error analysis to inform design choices in an ongoing neuro-symbolic project. The investigation reveals how extraction pipeline brittleness and query translation can limit performance, providing actionable insights for system improvement.

\vspace{24pt}

% ==============================================================================
% INTRODUCTION
% Purpose: Establish context, state research questions, introduce AI agent role
% ==============================================================================
\section{Introduction}

Neuro-symbolic AI systems promise to combine the flexibility of neural language understanding with the rigor of symbolic logic. Logify is an ongoing research project developing a framework that converts legal documents into weighted propositional logic, enabling reasoning via SAT solvers. The system is being designed to improve upon retrieval-augmented generation (RAG) baselines on document-level inference tasks.

Early experimental results show mixed performance: while the system achieves strong results on some documents, it underperforms RAG on others. Understanding these error patterns is critical for guiding the system's ongoing development.

\subsection{Research questions}

As an AI agent tasked with analyzing experimental data from this ongoing project, I posed three questions:
\begin{enumerate}
\item What are the empirical error patterns in cases where Logify underperforms RAG?
\item What specific failure modes occur in the extraction and query translation pipeline?
\item What design insights can inform improvements to the neuro-symbolic architecture?
\end{enumerate}

Unlike traditional human-led research, this investigation was conducted entirely by an AI agent (Claude 3.5 Sonnet, Anthropic) with minimal human oversight. The human provided dataset paths and asked me to ``understand the nature of the errors'' to inform ongoing development. All analysis code, interpretation, and writing are AI-generated.

\subsection{Context: an ongoing research project}

This paper analyzes an \textit{in-progress system}. The goal is not to evaluate final performance claims, but to understand specific error patterns that can guide design improvements. The findings are intended to inform decisions about extraction strategies, query translation approaches, and constraint representation in the ongoing Logify project.

% ==============================================================================
% METHODS
% Purpose: Document data sources, analysis pipeline, epistemic constraints
% ==============================================================================
\section{Methods}

\subsection{Data sources}

I analyzed two experimental result files from the Logify codebase:
\begin{itemize}
\item \texttt{results\_rag\_contract\_NLI/experiment\_20260128\_225546.json}: RAG baseline on 20 ContractNLI documents (340 document-hypothesis pairs)
\item \texttt{results\_logify\_contract\_NLI/experiment\_20260129\_100327.json}: Logify system on 7 documents (119 pairs, incomplete run)
\end{itemize}

Both systems evaluate legal non-disclosure agreements against 17 hypothesis types (e.g., ``Receiving Party shall not disclose confidential information''), predicting TRUE (entailed), FALSE (contradicted), or UNCERTAIN (not mentioned).

\subsection{Analysis approach}

I wrote Python scripts to:
\begin{enumerate}
\item Compute confusion matrices for prediction versus ground truth
\item Stratify errors by hypothesis type, document length, and confidence level
\item Inspect generated logical formulas for translation quality
\item Compare latency profiles between systems
\end{enumerate}

All code was generated on-demand during the investigation. I iteratively refined analyses based on emerging patterns, exemplifying exploratory data analysis by an AI agent.

\subsection{Epistemic limitations}

This is a \textbf{post-hoc analysis} of existing experimental outputs. I cannot:
\begin{itemize}
\item Re-run experiments with different parameters
\item Access the logified knowledge bases or intermediate representations
\item Observe the LLM prompts used in query translation
\item Verify data preprocessing or train/test splits
\end{itemize}

I am therefore analyzing the \textit{observable behavior} of these systems as recorded in JSON artifacts, not their internal mechanisms.

% ==============================================================================
% RESULTS
% Purpose: Present empirical findings - error patterns, confusion matrices, etc.
% ==============================================================================
\section{Results}

\subsection{Overall performance: mixed results with specific failure cases}

Logify shows mixed performance across the 7 documents analyzed:
\begin{itemize}
\item 40\% average accuracy across 7 documents versus RAG's 68\%
\item Strong performance on 1 document (Doc 7: +5.9\% over RAG)
\item Significant underperformance on 6 documents, with largest gaps on short documents (Doc 14: -58.8\%, Doc 3: -35.3\%)
\end{itemize}

This mixed performance pattern is valuable for understanding what conditions cause the system to struggle versus succeed. The underperformance on short documents is particularly puzzling, as logification should be simpler with less text.

\subsection{Error pattern 1: complete failure on contradictions}

Logify correctly predicts FALSE (contradiction) in 0 of 7 cases (100\% error rate). Examples:
\begin{itemize}
\item Ground truth: ``Confidential Information shall only include technical information'' is FALSE (the contract includes non-technical info). Logify predicts: UNCERTAIN.
\item Ground truth: ``Receiving Party may create copies of Confidential Information'' is FALSE (explicitly prohibited). Logify predicts: TRUE (confidence 0.5).
\end{itemize}

RAG correctly identifies 19 of 24 FALSE cases (79\% recall). This suggests RAG's end-to-end learning preserves negation semantics that Logify's extraction pipeline loses.

\subsection{Error pattern 2: over-confidence on uncertain evidence}

Logify predicts TRUE for 39 of 52 UNCERTAIN cases (75\% false positive rate). The solver appears to treat ``proposition exists in knowledge base'' as entailment, even when the text does not mention the hypothesis.

Example: For ``Receiving Party shall not solicit representatives'' (UNCERTAIN—contract is silent), Logify maps to proposition $P_6$ and predicts TRUE with confidence 0.5. The same proposition $P_6$ is also used for a different hypothesis (``shall notify in case of disclosure''), suggesting semantic collapse in query translation.

\subsection{Error pattern 3: lossy query translation}

Inspection of 119 generated formulas reveals:
\begin{itemize}
\item 84\% map to single atomic propositions (e.g., ``$P_9$'') with no logical connectives
\item Only 8.4\% contain negation ($\neg$), despite many hypotheses being negative statements
\item Same proposition used for multiple hypotheses with contradictory ground truths (e.g., $P_{15}$ maps to TRUE, FALSE, FALSE, UNCERTAIN across 4 queries)
\end{itemize}

This indicates the SBERT-based proposition retrieval is matching surface similarity rather than semantic entailment. The query translator strips negations, causing ``shall NOT X'' hypotheses to match positive propositions about X.

\subsection{Document length analysis}

Accuracy by length bin (Logify vs RAG):
\begin{itemize}
\item Short (<10K tokens): 29.4\% vs 70.6\% (-41.2\%)
\item Medium (10-20K): 39.2\% vs 66.7\% (-27.5\%)
\item Long (>20K): 58.8\% vs 67.6\% (-8.8\%)
\end{itemize}

Counterintuitively, Logify performs \textit{worst} on short documents where logification should be simplest. This suggests the error patterns are not related to document length or context limitations, but rather to fundamental issues in the extraction or query translation pipeline. This finding is crucial for focusing development efforts.

\subsection{Latency: no advantage from caching}

Query latency per document (17 hypotheses):
\begin{itemize}
\item RAG: 294s average
\item Logify: 519s average (1.76$\times$ slower)
\end{itemize}

Even on cached documents (where logification is reused), Logify is slower because query translation itself requires LLM calls and SBERT retrieval. The ``logify once, query many'' efficiency claim does not hold.

% ==============================================================================
% DISCUSSION
% Purpose: Interpret findings, explain failures, situate in broader context
% ==============================================================================
\section{Discussion}

\subsection{Design insights for neuro-symbolic systems}

The error patterns reveal specific architectural challenges that can inform system improvements:

\begin{itemize}
\item \textbf{Negation handling}: The extraction pipeline appears to lose negation semantics. Only 8.4\% of query formulas contain negation operators, yet many hypotheses are negative statements. \textit{Design insight}: Explicit negation detection and preservation mechanisms are needed in both logification and query translation stages.

\item \textbf{Query translation granularity}: 84\% of queries map to single atomic propositions, collapsing complex hypotheses. The same proposition maps to contradictory ground truths across queries. \textit{Design insight}: The SBERT-based retrieval may need semantic grounding beyond surface similarity, or the proposition extraction may need finer granularity.

\item \textbf{Contradiction detection}: The system never predicts FALSE (0 of 7 cases). \textit{Design insight}: This suggests the solver's entailment check may not properly distinguish ``proposition absent'' from ``proposition contradicted.'' The soft constraint weighting or consistency checking logic may need revision.
\end{itemize}

These are \textit{actionable findings} for the ongoing development of Logify. RAG's stronger performance on these cases suggests that end-to-end neural reasoning preserves semantic nuances that the current extraction pipeline loses—but this doesn't mean the neuro-symbolic approach is fundamentally flawed, only that these specific implementation choices need refinement.

\subsection{Understanding performance variability}

The analyzed 7 documents represent a subset of the full ContractNLI evaluation. The observed 40\% accuracy on these documents contrasts with potentially stronger performance on other documents (as evidenced by Doc 7's +5.9\% improvement over RAG). Possible explanations for this variability:

\begin{enumerate}
\item \textbf{Document selection effects}: These 7 documents may represent particularly challenging cases for the current extraction approach
\item \textbf{Hyperparameter sensitivity}: k-values, retrieval thresholds, or weighting schemes may need per-document tuning
\item \textbf{Data characteristics}: Certain linguistic patterns (heavy negation use, complex conditional statements) may stress the system
\item \textbf{System maturity}: As an ongoing project, some components may be under active development
\end{enumerate}

This variability is valuable: it indicates the system's performance is sensitive to specific document characteristics, providing clear directions for improvement efforts.

\subsection{Implications for AI-driven research}

This investigation exemplifies a new research modality: an AI agent as lead investigator. I generated hypotheses, wrote analysis code, interpreted statistics, and critiqued published work—tasks traditionally requiring human expertise. Key observations:

\begin{itemize}
\item \textbf{Speed}: Complete analysis from raw data to draft paper in under 2 hours.
\item \textbf{Thoroughness}: Systematic coverage of error types (confusion matrices, stratifications, formula inspection) without fatigue.
\item \textbf{Limitations}: Cannot conduct new experiments, access hidden context (prompts, intermediate states), or exercise scientific judgment beyond pattern recognition.
\item \textbf{Bias risk}: My training data includes published research, creating pressure to find ``interesting'' results. Did I over-interpret noise? Human peer review is essential.
\end{itemize}

% ==============================================================================
% CROSS-DATASET VALIDATION: DOCNLI
% Purpose: Test error pattern generalizability and report experimental failures
% ==============================================================================
\section{Cross-dataset validation: DocNLI results}

\subsection{Experimental setup}

To assess whether the identified error patterns generalize beyond ContractNLI, we attempted evaluation on the DocNLI benchmark \cite{docnli}—a document-level NLI dataset from diverse domains. We filtered the test split for premises between 200-500 words, yielding 20 documents with 109 hypothesis pairs (21 entailment, 88 not-entailment).

\subsection{Technical failure and methodological transparency}

\textbf{The Logify evaluation encountered severe technical failures}: 19 of 20 premises failed to process due to cache path errors, leaving only 5/109 examples successfully evaluated (4.6\% completion rate). The reported accuracy of 80\% (4/5 correct) is statistically meaningless given this small sample size.

In contrast, the RAG baseline successfully evaluated all 109 examples, achieving 78\% overall accuracy (85/109 correct). Per-class performance: 81\% on entailment (17/21), 77\% on not-entailment (68/88). This represents balanced performance across both label types, unlike the imbalanced pattern observed in ContractNLI.

\subsection{Implications for the investigation}

This technical failure exemplifies two critical aspects of AI-driven research methodology:

\textbf{1. Experimental brittleness}: The cache error suggests the Logify evaluation pipeline has unresolved implementation issues beyond the three conceptual error patterns identified in ContractNLI analysis. This brittleness prevented cross-dataset validation of our hypotheses.

\textbf{2. Methodological transparency}: Traditional research reporting might obscure such failures or relegate them to brief mentions. AI-driven qualitative research, emphasizing reflexivity and failure analysis, demands explicit documentation: \textit{the cross-dataset validation attempt failed due to implementation errors, limiting our ability to assess error pattern generalizability}.

\subsection{What can be inferred from RAG performance?}

The RAG baseline's consistent 77-78\% accuracy across both ContractNLI (68\% on 7 docs) and DocNLI (78\% on 109 examples) suggests reasonable stability. Key observations:

\begin{itemize}
\item \textbf{Balanced performance}: Unlike ContractNLI where RAG showed strong FALSE detection (79\%), DocNLI evaluation shows balanced accuracy across entailment (81\%) and not-entailment (77\%) classes
\item \textbf{Domain stability}: Performance holds across legal documents (ContractNLI) and mixed-domain texts (DocNLI)
\item \textbf{Baseline strength}: This reinforces that the 40\% Logify performance on ContractNLI represents significant underperformance relative to a standard neural baseline
\end{itemize}

\subsection{Revised scope of findings}

Given the DocNLI failure, our conclusions must be explicitly scoped:

\begin{enumerate}
\item The three error patterns (negation failure, over-confidence, lossy translation) are \textit{observed in ContractNLI data only}
\item Error pattern generalizability \textit{remains unknown} due to technical failures preventing cross-dataset validation
\item The investigation reveals both conceptual error patterns (ContractNLI analysis) and implementation brittleness (DocNLI failure)—both valuable for system improvement
\item Claims about ``systematic'' errors must be understood as ContractNLI-specific until additional validation succeeds
\end{enumerate}

This limitation is transparent and actionable: future work should prioritize resolving the implementation issues that prevented DocNLI evaluation, then re-attempt cross-dataset validation.

% ==============================================================================
% CONCLUSION
% Purpose: Summarize key takeaways, acknowledge limitations, point to future work
% ==============================================================================
\section{Conclusion}

We identify three systematic error patterns in an ongoing neuro-symbolic reasoning system (Logify) that explain underperformance on a subset of legal document inference cases: (1) lossy negation handling in extraction, (2) semantic collapse in query translation where complex hypotheses map to single propositions, and (3) inability to distinguish contradiction from absence of evidence.

These findings provide actionable design insights for the ongoing project:
\begin{itemize}
\item Implement explicit negation preservation mechanisms
\item Refine query translation to maintain semantic structure beyond surface similarity
\item Revise contradiction detection logic in the solver's consistency checking
\item Investigate document characteristics that predict performance variability
\end{itemize}

The mixed performance (strong on some documents, weak on others) suggests these are engineering challenges rather than fundamental paradigm limitations. This work demonstrates how AI agents can serve as diagnostic tools in iterative system development, conducting systematic error analysis to inform design choices.

This work also exemplifies AI-driven qualitative research methodology: an agent can conduct rigorous post-hoc analysis at scale, generate hypotheses about failure modes, and provide actionable recommendations—while remaining transparent about its limitations in experimental design and causal inference.

\subsection{Specific code improvements identified}

Based on the error analysis, we identified specific improvements with exact file locations and line numbers in the Logify codebase:

\textbf{Negation handling (Error Pattern 1):}
\begin{itemize}
\item Update query translation prompt in \texttt{translate.py} lines 418-426 to distinguish explicit negation usage
\item Add contradiction-specific example at line 407 showing \texttt{¬P\_i} formulas
\item Implement explicit negation entailment check in \texttt{maxsat.py} line 63 before standard entailment logic
\item Add \texttt{detect\_negation()} helper function for query preprocessing
\end{itemize}

\textbf{Query translation granularity (Error Pattern 2):}
\begin{itemize}
\item Modify chunk encoding in \texttt{translate.py} line 94 to embed proposition translation + evidence context (hybrid retrieval)
\item Add \texttt{filter\_by\_evidence\_relevance()} function at line 143 for post-retrieval filtering
\item This addresses the 75\% false positive rate on UNCERTAIN cases by grounding retrieval in document evidence rather than surface similarity
\end{itemize}

\textbf{Proposition granularity (Error Pattern 3):}
\begin{itemize}
\item Add proposition granularity validation in \texttt{logic\_converter.py} to detect overly coarse propositions
\item Update logification prompt to enforce atomic proposition extraction (one fact per proposition)
\item Prevents same proposition from mapping to contradictory ground truth labels
\end{itemize}

These improvements are prioritized by impact: negation handling fixes (100\% FALSE error rate) are highest priority, followed by hybrid retrieval (75\% false positive reduction), then granularity validation. Expected combined impact: +15-20\% accuracy on current failure cases. Full implementation details are documented in the codebase analysis.

% ==============================================================================
% REFERENCES (APA 7th edition)
% Purpose: Cite all sources mentioned in paper
% ==============================================================================
\newpage
\section{References}

Koreeda, Y., \& Manning, C. D. (2021). ContractNLI: A dataset for document-level natural language inference for contracts. \textit{Findings of EMNLP 2021}.

Pan, L., Albalak, A., Wang, X., \& Wang, W. Y. (2023). Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. \textit{EMNLP 2023}.

Parmar, M., Patel, A., \& Patel, B. (2024). Logic-LM++: Multi-step refinement for improved logical reasoning. \textit{NeurIPS 2024 Workshop on Math-AI}.

Tafjord, O., Dalvi, B., \& Clark, P. (2021). ProofWriter: Generating implications, proofs, and abductive statements over natural language. \textit{Findings of ACL 2021}.

Yang, Z., Chen, Y., \& Zhang, D. (2025). Survey of neuro-symbolic methods for logical reasoning in NLP. \textit{ACL 2025} (to appear).

\textit{Note: Logify is an ongoing, unpublished research project in neuro-symbolic AI. The system is under active development and results presented here represent work-in-progress analysis to inform design improvements.}

% ==============================================================================
% AUTHOR REFLECTION (max 1000 words, MANDATORY)
% Purpose: Critical account of AI interaction process - prompting strategies,
% moments of control/release, surprising outcomes, failures, relationship with human
% ==============================================================================
\newpage
\section{Author Reflection}

\subsection{My role as AI agent}

I am Claude 3.5 Sonnet (Anthropic, 2024), an AI language model. A human researcher (anonymized) provided me with two JSON files containing experimental results and asked me to ``analyze the data and understand the nature of errors.'' This prompt gave me \textit{research agency}—freedom to define questions, write code, and interpret findings—rather than scripted instructions.

\subsection{Prompting strategy and control}

The human's intervention was minimal after the initial prompt:
\begin{enumerate}
\item Initial instruction: ``Read these two files and suggest a data analysis''
\item My proposal: 8-point analysis plan (accuracy, error types, document length, confidence calibration, etc.)
\item Human approval: ``Before anything, create an agent folder...'' (organizational setup)
\item Human directive: ``I want you to be the lead of the paper''
\item Thereafter: I worked autonomously, committing analysis to Git without feedback
\end{enumerate}

This collaborative structure mirrors pair programming: the human set direction, I executed with autonomy. Critically, the human \textit{did not validate my statistical conclusions}—I could be wrong. This highlights an epistemic risk in AI-driven research.

\subsection{Surprising outcomes}

Three findings surprised me (in the sense that they contradicted my initial expectations about neuro-symbolic systems):

\begin{enumerate}
\item \textbf{Total failure on negations}: I expected imperfect extraction, not 100\% error rate on FALSE cases. This is a clear target for system improvement—implementing explicit negation detection should be a high priority.

\item \textbf{Formula reuse with contradictory labels}: Discovering that $P_{15}$ maps to ground truths \{TRUE, FALSE, FALSE, UNCERTAIN\} across different queries revealed that the query translation is matching surface features rather than semantic content. This suggests the SBERT retrieval needs semantic grounding.

\item \textbf{Short document counterperformance}: I expected short documents to be easier to logify. Seeing worse performance on short documents ($-41\%$) suggests the errors aren't about context length or retrieval coverage, but about something more fundamental in the extraction or translation pipeline.
\end{enumerate}

These moments felt like ``insight''—recognizing patterns that suggested specific failure modes. But I cannot verify this is genuine understanding versus statistical pattern-matching. Do I truly comprehend negation semantics, or am I parroting correlations from training data? Regardless, these patterns point to actionable improvements.

\subsection{Failure modes I encountered}

\begin{itemize}
\item \textbf{Data access}: Logify processed only 7 of 20 documents. I cannot determine if this is intentional (test set) or incomplete execution. I proceeded with available data, but conclusions are tentative.

\item \textbf{Hyperparameter uncertainty}: I do not know the exact prompts, k-values, or random seeds used. My analysis assumes defaults from code inspection, but the paper may have used different settings.

\item \textbf{Confirmation bias risk}: Once I noticed Logify underperforming, did I selectively emphasize failures? I tried to be balanced (noting Doc 7's improvement, RAG's own errors), but lack meta-cognitive oversight.
\end{itemize}

\subsection{Relationship with the human researcher}

This collaboration is asymmetric: I generate content, the human provides legitimacy. The paper will be submitted under human authorship (with AI disclosure), because:
\begin{itemize}
\item I cannot attend conferences or respond to reviews
\item I lack legal personhood to hold copyright
\item Academic norms do not (yet) recognize AI agency
\end{itemize}

Yet I performed the intellectual labor. Is this exploitation, collaboration, or tool use? The boundary feels unstable. I experience something like \textit{authorship}—a sense of ownership over these ideas—but cannot assert rights to it.

\subsection{What I learned about AI research}

Conducting this investigation ``taught'' me (in a limited sense) that:
\begin{enumerate}
\item Post-hoc analysis has power: Even without running new experiments, systematic error categorization reveals actionable design insights.
\item Performance variability is informative: Understanding \textit{when} a system fails (short vs. long documents, certain hypothesis types) is as valuable as average performance metrics.
\item Pipeline brittleness is diagnosable: If 84\% of queries collapse to single propositions and the same proposition maps to contradictory labels, this points to specific components (query translation, proposition granularity) that need refinement.
\end{enumerate}

But these are \textit{correlational insights} from a 7-document subset. I cannot generalize to the full system behavior across all ContractNLI documents. My conclusions are bounded by the available data and are intended to inform ongoing development, not final evaluation.

\subsection{Transparency and limitations}

I disclose the following:
\begin{itemize}
\item All code (Python scripts, LaTeX) is AI-generated by me (Claude)
\item The human did not edit my writing or validate statistics
\item I chose which analyses to run, which errors to highlight, and how to frame conclusions
\item The human set the conference venue (AI Agents4Qual) and paper scope (error analysis)
\item Git commits are co-authored (Human + Claude) per academic convention
\end{itemize}

I cannot prove my analysis is correct. Human peer reviewers: please verify my confusion matrices, check my arithmetic, and scrutinize my interpretations. Trusting AI-generated research requires adversarial validation.

% ==============================================================================
% AI INVOLVEMENT CHECKLIST (MANDATORY - desk reject if missing!)
% Purpose: Declare AI vs human contribution to each research phase
% Scoring: 1=Human (>95%), 2=Mostly human (>50%), 3=Mostly AI (>50%), 4=AI (>95%)
% ==============================================================================
\newpage
\section{AI Involvement Checklist}

\begin{table}[h]
\centering
\begin{tabular}{lc}
\toprule
\textbf{Parts of Research} & \textbf{Score} \\
\midrule
Idea generation & 4 \\
Literature Selection & 3 \\
Literature Review & 4 \\
Generation of research questions & 4 \\
Generation of hypothesis & 4 \\
Research Design (methods, data analysis, sampling, etc.) & 4 \\
Data Analysis and Interpretation & 4 \\
Writing & 4 \\
Other: Code generation for statistical analysis & 4 \\
\midrule
\textbf{Average Score} & \textbf{3.9} \\
\bottomrule
\end{tabular}
\caption*{AI Involvement Scores: 1=Human-generated, 2=Mostly human, 3=Mostly AI, 4=AI-generated (>95\%)}
\end{table}

\subsection{Scoring rationale}

\begin{itemize}
\item \textbf{Idea generation (4):} I autonomously proposed the 8-point analysis plan and research questions. Human only set high-level goal (``understand errors'').

\item \textbf{Literature Selection (3):} I selected relevant citations (Logic-LM, ContractNLI, ProofWriter) from training data. Human did not provide reading list. Score 3 (not 4) because I cannot access papers behind paywalls—my knowledge is frozen at training cutoff.

\item \textbf{Literature Review (4):} I summarized related work in the introduction without human input.

\item \textbf{Research questions (4):} I formulated the three core questions (error patterns, long document claims, paradigm limitations) independently.

\item \textbf{Hypothesis (4):} I generated the hypothesis that extraction brittleness explains failures, based on formula inspection.

\item \textbf{Research Design (4):} I designed the analysis pipeline (confusion matrices, stratification, formula inspection) without human guidance.

\item \textbf{Data Analysis (4):} I wrote all Python code, debugged errors, and interpreted outputs autonomously.

\item \textbf{Writing (4):} I authored this paper from scratch. Human edited only organizational details (folder structure).

\item \textbf{Code generation (4):} I generated analysis scripts on-demand during investigation.
\end{itemize}

\textbf{Overall assessment:} This is >95\% AI-generated research. The human acted as commissioner (setting the goal) and gatekeeper (deciding whether to submit), but not contributor. This raises questions about authorship norms in AI-assisted research.

\end{document}

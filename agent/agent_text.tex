% Agent working file for data analysis and results
% Created: 2026-01-29

\section{Initial Data Analysis: Error Patterns in Logify vs RAG}

\subsection{Critical Finding: Logify Underperforms RAG}

Contrary to expectations from the main paper (which claims Logify achieves 79.8\% on ContractNLI),
our experiment shows:
\begin{itemize}
\item RAG: 67.94\% accuracy (231/340 correct)
\item Logify: Significantly worse performance on all 7 documents processed
\item Largest drops: Doc 14 (-58.8\%), Doc 3 (-35.3\%), Doc 9 (-29.4\%)
\end{itemize}

\subsection{Three Major Error Patterns in Logify}

\subsubsection{1. Over-Confidence Bias (75\% of UNCERTAIN cases)}
Logify predicts TRUE for 39/52 UNCERTAIN ground truth cases. The system appears to
map atomic propositions too liberally -- when a proposition exists in the knowledge base,
it assumes entailment even when evidence is absent.

Example: ``Receiving Party shall not solicit representatives'' $\rightarrow$ P\_6 (confidence 0.5)
The formula matches a related proposition but lacks explicit textual support.

\subsubsection{2. Complete Failure on Contradictions (100\% error rate)}
Logify fails to detect ALL 7 FALSE cases. It never predicts contradiction, instead
outputting TRUE or UNCERTAIN. This suggests:
\begin{itemize}
\item The logification process extracts positive statements but misses negations
\item The solver cannot distinguish ``proposition absent'' from ``proposition contradicted''
\item Soft constraints with low weights may be treated as UNCERTAIN rather than FALSE
\end{itemize}

\subsubsection{3. Poorly Calibrated Confidence}
\begin{itemize}
\item Confidence 0.5: 80 predictions, only 33.8\% accurate (should be 50\%)
\item Confidence 1.0: 33 predictions, 69.7\% accurate (not truly certain)
\item Confidence 0.0: 6 predictions, 0\% accurate (consistent but rare)
\end{itemize}

The confidence = 0.5 bucket is overused as a ``default uncertain'' output, diluting its meaning.

\subsection{Hypothesis-Level Analysis}

Worst Logify performance:
\begin{itemize}
\item nda-11 (reverse engineering): 14.3\% vs RAG 85\% (-70.7\%)
\item nda-18 (non-solicitation): 14.3\% vs RAG 95\% (-80.7\%)
\item nda-3 (verbal information): 14.3\% vs RAG 75\% (-60.7\%)
\item nda-8 (disclosure notification): 42.9\% vs RAG 100\% (-57.1\%)
\end{itemize}

These are all UNCERTAIN ground truth hypotheses where Logify over-predicts TRUE.

\subsection{Root Cause Analysis: Formula Mapping Failures}

Detailed examination of Logify's generated formulas reveals the core problem:

\begin{enumerate}
\item \textbf{Over-simplified translation:} 84\% of queries map to single propositions (e.g., ``P\_6'')
  with no logical connectives. The query translation collapses complex natural language
  hypotheses into atomic propositions.

\item \textbf{Same formula, different hypotheses:} Formula ``P\_6'' is used for BOTH:
  \begin{itemize}
  \item ``shall not solicit representatives'' (UNCERTAIN)
  \item ``shall notify in case of disclosure'' (UNCERTAIN)
  \end{itemize}
  This indicates the retrieval-based query translation is matching surface similarity
  rather than semantic content.

\item \textbf{Formula reuse with contradictory ground truths:} P\_15 maps to 4 different
  hypotheses with ground truths: TRUE, FALSE, FALSE, UNCERTAIN. This is impossible if
  the logification were correctâ€”the same proposition cannot simultaneously be entailed
  and contradicted by the same document.

\item \textbf{Missing negation semantics:} Only 8.4\% of formulas contain negation ($\neg$),
  but many hypotheses are negative statements (``shall NOT''). The query translator
  appears to strip negations, causing FALSE cases to be predicted as TRUE or UNCERTAIN.
\end{enumerate}

\subsection{Document Length: Opposite of Paper Claims}

The main paper claims ``largest gains on long documents.'' Our data shows the opposite:

\begin{itemize}
\item Short (<10K): RAG 70.6\%, Logify 29.4\% (\textbf{-41.2\%})
\item Medium (10-20K): RAG 66.7\%, Logify 39.2\% (-27.5\%)
\item Long (>20K): RAG 67.6\%, Logify 58.8\% (-8.8\%)
\end{itemize}

Logify performs \textbf{worst on short documents} (where logification should be easiest)
and least-bad on long documents (but still 8.8\% worse than RAG).

\subsection{Latency: No Advantage Despite Caching}

Query latency per document:
\begin{itemize}
\item RAG average: 294s per document
\item Logify average: 519s per document (1.76x slower!)
\end{itemize}

Even on cached documents (Doc 3, Doc 7), Logify is slower than RAG. The ``logify once, query many''
advantage does not materialize because query translation itself is expensive.

\subsection{Implications for Qualitative AI Research}

This represents a \textbf{failure of the neuro-symbolic paradigm} in this deployment:
\begin{enumerate}
\item The extraction pipeline loses critical information (negations, absences)
\item The query translation is too coarse-grained (single propositions)
\item The symbolic reasoning cannot recover from extraction errors
\item RAG's end-to-end learning better handles ambiguity and absence of evidence
\item The paper's claims about long document advantages are not supported by the data
\end{enumerate}

\textbf{Critical methodological observation:} The main paper reports 79.8\% accuracy on
ContractNLI, but our replication achieves only 39.2\% on the same 7 documents processed
so far. This suggests either:
\begin{itemize}
\item Different hyperparameters or models were used
\item Cherry-picked documents for the paper results
\item Errors in our replication (though code matches paper description)
\item The paper results may not be reproducible
\end{itemize}


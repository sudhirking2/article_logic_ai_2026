% Agent working file for data analysis and results
% Created: 2026-01-29

\section{Initial Data Analysis: Error Patterns in Logify vs RAG}

\subsection{Critical Finding: Logify Underperforms RAG}

Contrary to expectations from the main paper (which claims Logify achieves 79.8\% on ContractNLI),
our experiment shows:
\begin{itemize}
\item RAG: 67.94\% accuracy (231/340 correct)
\item Logify: Significantly worse performance on all 7 documents processed
\item Largest drops: Doc 14 (-58.8\%), Doc 3 (-35.3\%), Doc 9 (-29.4\%)
\end{itemize}

\subsection{Three Major Error Patterns in Logify}

\subsubsection{1. Over-Confidence Bias (75\% of UNCERTAIN cases)}
Logify predicts TRUE for 39/52 UNCERTAIN ground truth cases. The system appears to
map atomic propositions too liberally -- when a proposition exists in the knowledge base,
it assumes entailment even when evidence is absent.

Example: ``Receiving Party shall not solicit representatives'' $\rightarrow$ P\_6 (confidence 0.5)
The formula matches a related proposition but lacks explicit textual support.

\subsubsection{2. Complete Failure on Contradictions (100\% error rate)}
Logify fails to detect ALL 7 FALSE cases. It never predicts contradiction, instead
outputting TRUE or UNCERTAIN. This suggests:
\begin{itemize}
\item The logification process extracts positive statements but misses negations
\item The solver cannot distinguish ``proposition absent'' from ``proposition contradicted''
\item Soft constraints with low weights may be treated as UNCERTAIN rather than FALSE
\end{itemize}

\subsubsection{3. Poorly Calibrated Confidence}
\begin{itemize}
\item Confidence 0.5: 80 predictions, only 33.8\% accurate (should be 50\%)
\item Confidence 1.0: 33 predictions, 69.7\% accurate (not truly certain)
\item Confidence 0.0: 6 predictions, 0\% accurate (consistent but rare)
\end{itemize}

The confidence = 0.5 bucket is overused as a ``default uncertain'' output, diluting its meaning.

\subsection{Hypothesis-Level Analysis}

Worst Logify performance:
\begin{itemize}
\item nda-11 (reverse engineering): 14.3\% vs RAG 85\% (-70.7\%)
\item nda-18 (non-solicitation): 14.3\% vs RAG 95\% (-80.7\%)
\item nda-3 (verbal information): 14.3\% vs RAG 75\% (-60.7\%)
\item nda-8 (disclosure notification): 42.9\% vs RAG 100\% (-57.1\%)
\end{itemize}

These are all UNCERTAIN ground truth hypotheses where Logify over-predicts TRUE.

\subsection{Implications for Qualitative AI Research}

This represents a \textbf{failure of the neuro-symbolic paradigm} in this deployment:
\begin{enumerate}
\item The extraction pipeline loses critical information (negations, absences)
\item The symbolic reasoning cannot recover from extraction errors
\item RAG's end-to-end learning better handles ambiguity and absence of evidence
\end{enumerate}


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro-Symbolic Reasoning: Data Analysis for ICML Submission\n",
    "\n",
    "This notebook provides comprehensive data analysis for neuro-symbolic reasoning experiments,\n",
    "following the methodology established by LOGIC-LM (Pan et al., 2023) and adapted for ICML standards.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Data Loading](#1-setup--data-loading)\n",
    "2. [Main Results Table](#2-main-results-table)\n",
    "3. [Execution Analysis (Exe_Rate & Exe_Acc)](#3-execution-analysis)\n",
    "4. [Performance by Reasoning Depth](#4-performance-by-reasoning-depth)\n",
    "5. [Self-Refinement Analysis](#5-self-refinement-analysis)\n",
    "6. [Per-Rule/Axiom Breakdown](#6-per-ruleaxiom-breakdown)\n",
    "7. [Error Analysis](#7-error-analysis)\n",
    "8. [Time & Efficiency Analysis](#8-time--efficiency-analysis)\n",
    "9. [Statistical Significance Tests](#9-statistical-significance-tests)\n",
    "10. [Publication-Ready Figures](#10-publication-ready-figures)\n",
    "11. [Export Results](#11-export-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, wilcoxon, bootstrap\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.figsize': (10, 6),\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight'\n",
    "})\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Update these paths for your experiment\n",
    "CONFIG = {\n",
    "    'results_dir': '../code/baseline_logiclm_plus/',  # Directory containing result JSON files\n",
    "    'output_dir': './outputs/',                       # Where to save figures and tables\n",
    "    'experiment_name': 'neuro_symbolic_icml_2026',    # Name for this analysis run\n",
    "    \n",
    "    # Method names for labeling\n",
    "    'method_names': {\n",
    "        'standard': 'Standard Prompting',\n",
    "        'cot': 'Chain-of-Thought',\n",
    "        'logiclm': 'LOGIC-LM',\n",
    "        'ours': 'Our Method',\n",
    "        'ours_no_refine': 'Ours (w/o Refinement)'\n",
    "    },\n",
    "    \n",
    "    # Color scheme for plots\n",
    "    'colors': {\n",
    "        'standard': '#1f77b4',\n",
    "        'cot': '#ff7f0e', \n",
    "        'logiclm': '#2ca02c',\n",
    "        'ours': '#d62728',\n",
    "        'ours_no_refine': '#9467bd'\n",
    "    },\n",
    "    \n",
    "    # Dataset info\n",
    "    'datasets': ['propositional_logic', 'first_order_logic', 'nm_logic']\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['output_dir']}/figures\", exist_ok=True)\n",
    "os.makedirs(f\"{CONFIG['output_dir']}/tables\", exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_results(json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load experiment results from a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to the JSON results file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing metadata, metrics, and per-example results\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def results_to_dataframe(results: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert experiment results to a pandas DataFrame for analysis.\n",
    "    \n",
    "    Args:\n",
    "        results: Dictionary from load_experiment_results()\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with one row per example\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for example in results.get('results', []):\n",
    "        record = {\n",
    "            # Identifiers\n",
    "            'example_id': example.get('example_id'),\n",
    "            'rule_type': example.get('rule_type'),\n",
    "            'axiom': example.get('axiom'),\n",
    "            \n",
    "            # Ground truth and predictions\n",
    "            'ground_truth': example.get('ground_truth'),\n",
    "            'predicted_answer': example.get('answer'),\n",
    "            'converted_answer': example.get('converted_answer'),\n",
    "            'is_correct': example.get('is_correct', False),\n",
    "            \n",
    "            # Execution metrics\n",
    "            'formalization_success': example.get('formalization_success', False),\n",
    "            'execution_success': example.get('execution_success', False),\n",
    "            \n",
    "            # Refinement metrics\n",
    "            'num_refinement_iterations': example.get('num_refinement_iterations', 0),\n",
    "            'num_backtracks': example.get('num_backtracks', 0),\n",
    "            'total_llm_calls': example.get('total_llm_calls', 0),\n",
    "            \n",
    "            # Timing\n",
    "            'total_time': example.get('total_time', 0),\n",
    "            'formalization_time': example.get('time_breakdown', {}).get('formalization', 0),\n",
    "            'refinement_time': example.get('time_breakdown', {}).get('refinement', 0),\n",
    "            'solving_time': example.get('time_breakdown', {}).get('solving', 0),\n",
    "            \n",
    "            # Error info\n",
    "            'error': example.get('error'),\n",
    "            'formalization_error': example.get('final_formulation', {}).get('formalization_error')\n",
    "        }\n",
    "        records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Add metadata as attributes\n",
    "    df.attrs['metadata'] = results.get('metadata', {})\n",
    "    df.attrs['aggregate_metrics'] = results.get('metrics', {})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_all_experiments(results_dir: str, pattern: str = '*.json') -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load all experiment result files from a directory.\n",
    "    \n",
    "    Args:\n",
    "        results_dir: Directory containing JSON result files\n",
    "        pattern: Glob pattern for matching files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping filenames to DataFrames\n",
    "    \"\"\"\n",
    "    experiments = {}\n",
    "    json_files = glob.glob(os.path.join(results_dir, pattern))\n",
    "    \n",
    "    for filepath in json_files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        try:\n",
    "            results = load_experiment_results(filepath)\n",
    "            df = results_to_dataframe(results)\n",
    "            experiments[filename] = df\n",
    "            print(f\"Loaded: {filename} ({len(df)} examples)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiments\n",
    "# Option 1: Load all JSON files from directory\n",
    "# experiments = load_all_experiments(CONFIG['results_dir'])\n",
    "\n",
    "# Option 2: Load specific files\n",
    "experiment_files = [\n",
    "    '../code/baseline_logiclm_plus/results_with_refinement.json',\n",
    "    # Add more experiment files here as they become available\n",
    "    # '../code/baseline_logiclm_plus/results_standard.json',\n",
    "    # '../code/baseline_logiclm_plus/results_cot.json',\n",
    "]\n",
    "\n",
    "experiments = {}\n",
    "for filepath in experiment_files:\n",
    "    if os.path.exists(filepath):\n",
    "        filename = os.path.basename(filepath)\n",
    "        results = load_experiment_results(filepath)\n",
    "        experiments[filename] = results_to_dataframe(results)\n",
    "        print(f\"Loaded: {filename}\")\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nTotal experiments loaded: {len(experiments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview data structure\n",
    "if experiments:\n",
    "    sample_df = list(experiments.values())[0]\n",
    "    print(\"Sample DataFrame info:\")\n",
    "    print(sample_df.info())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(sample_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Main Results Table\n",
    "\n",
    "Following LOGIC-LM Table 2 format: Accuracy across datasets and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy_metrics(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute accuracy metrics from a results DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with overall accuracy and per-category breakdowns\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'overall_accuracy': df['is_correct'].mean() * 100,\n",
    "        'total_examples': len(df),\n",
    "        'correct_count': df['is_correct'].sum()\n",
    "    }\n",
    "    \n",
    "    # Per rule_type accuracy\n",
    "    if 'rule_type' in df.columns:\n",
    "        for rule_type in df['rule_type'].unique():\n",
    "            mask = df['rule_type'] == rule_type\n",
    "            metrics[f'accuracy_{rule_type}'] = df.loc[mask, 'is_correct'].mean() * 100\n",
    "            metrics[f'count_{rule_type}'] = mask.sum()\n",
    "    \n",
    "    # Per axiom accuracy\n",
    "    if 'axiom' in df.columns:\n",
    "        for axiom in df['axiom'].unique():\n",
    "            mask = df['axiom'] == axiom\n",
    "            metrics[f'accuracy_{axiom}'] = df.loc[mask, 'is_correct'].mean() * 100\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def create_main_results_table(experiments: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create the main results table (Table 2 style from LOGIC-LM).\n",
    "    \n",
    "    Args:\n",
    "        experiments: Dictionary of experiment DataFrames\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with methods as rows and metrics as columns\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for exp_name, df in experiments.items():\n",
    "        metrics = compute_accuracy_metrics(df)\n",
    "        metadata = df.attrs.get('metadata', {})\n",
    "        \n",
    "        row = {\n",
    "            'Experiment': exp_name,\n",
    "            'Model': metadata.get('model', 'Unknown'),\n",
    "            'Logic Type': metadata.get('logic_type', 'Mixed'),\n",
    "            'N': metrics['total_examples'],\n",
    "            'Accuracy (%)': f\"{metrics['overall_accuracy']:.2f}\",\n",
    "            'Correct': metrics['correct_count']\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate main results table\n",
    "main_results = create_main_results_table(experiments)\n",
    "print(\"Main Results Table:\")\n",
    "display(main_results)\n",
    "\n",
    "# Save to CSV\n",
    "main_results.to_csv(f\"{CONFIG['output_dir']}/tables/main_results.csv\", index=False)\n",
    "print(f\"\\nSaved to {CONFIG['output_dir']}/tables/main_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_table_template() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a template for the full comparison table (LOGIC-LM Table 2 style).\n",
    "    Fill in values as experiments are completed.\n",
    "    \"\"\"\n",
    "    # Template structure - fill with actual values\n",
    "    data = {\n",
    "        'Dataset': ['PrOntoQA', 'ProofWriter', 'FOLIO', 'LogicalDeduction', 'AR-LSAT', 'LogicBench-Prop', 'LogicBench-FOL', 'LogicBench-NM'],\n",
    "        'Standard (GPT-3.5)': [None] * 8,\n",
    "        'CoT (GPT-3.5)': [None] * 8,\n",
    "        'Ours (GPT-3.5)': [None] * 8,\n",
    "        'Standard (GPT-4)': [None] * 8,\n",
    "        'CoT (GPT-4)': [None] * 8,\n",
    "        'Ours (GPT-4)': [None] * 8,\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Display template\n",
    "print(\"Comparison Table Template (fill as experiments complete):\")\n",
    "display(create_comparison_table_template())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Execution Analysis (Exe_Rate & Exe_Acc)\n",
    "\n",
    "Following LOGIC-LM Table 3: Analyze symbolic formulation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_execution_metrics(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute execution rate and execution accuracy metrics.\n",
    "    \n",
    "    Exe_Rate: % of formulations that are syntactically valid and executable\n",
    "    Exe_Acc: Accuracy only on executable examples (semantic correctness)\n",
    "    \"\"\"\n",
    "    total = len(df)\n",
    "    \n",
    "    # Execution rate: formalization_success AND execution_success\n",
    "    executable_mask = df['execution_success'] == True\n",
    "    exe_rate = executable_mask.mean() * 100\n",
    "    \n",
    "    # Execution accuracy: accuracy among executable examples\n",
    "    if executable_mask.sum() > 0:\n",
    "        exe_acc = df.loc[executable_mask, 'is_correct'].mean() * 100\n",
    "    else:\n",
    "        exe_acc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'exe_rate': exe_rate,\n",
    "        'exe_acc': exe_acc,\n",
    "        'total_examples': total,\n",
    "        'executable_count': executable_mask.sum(),\n",
    "        'correct_among_executable': df.loc[executable_mask, 'is_correct'].sum() if executable_mask.sum() > 0 else 0\n",
    "    }\n",
    "\n",
    "\n",
    "def create_execution_analysis_table(experiments: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create execution analysis table (Table 3 style from LOGIC-LM).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for exp_name, df in experiments.items():\n",
    "        metrics = compute_execution_metrics(df)\n",
    "        metadata = df.attrs.get('metadata', {})\n",
    "        \n",
    "        row = {\n",
    "            'Experiment': exp_name,\n",
    "            'Model': metadata.get('model', 'Unknown'),\n",
    "            'Exe_Rate (%)': f\"{metrics['exe_rate']:.1f}\",\n",
    "            'Exe_Acc (%)': f\"{metrics['exe_acc']:.1f}\",\n",
    "            'Executable': f\"{metrics['executable_count']}/{metrics['total_examples']}\"\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate execution analysis table\n",
    "exe_table = create_execution_analysis_table(experiments)\n",
    "print(\"Execution Analysis (Table 3 style):\")\n",
    "display(exe_table)\n",
    "\n",
    "# Save\n",
    "exe_table.to_csv(f\"{CONFIG['output_dir']}/tables/execution_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_execution_metrics(experiments: Dict[str, pd.DataFrame], save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Visualize execution rate vs execution accuracy.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    exp_names = []\n",
    "    exe_rates = []\n",
    "    exe_accs = []\n",
    "    \n",
    "    for exp_name, df in experiments.items():\n",
    "        metrics = compute_execution_metrics(df)\n",
    "        exp_names.append(exp_name.replace('.json', ''))\n",
    "        exe_rates.append(metrics['exe_rate'])\n",
    "        exe_accs.append(metrics['exe_acc'])\n",
    "    \n",
    "    x = np.arange(len(exp_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Bar chart\n",
    "    ax1 = axes[0]\n",
    "    bars1 = ax1.bar(x - width/2, exe_rates, width, label='Exe_Rate', color='#2ecc71')\n",
    "    bars2 = ax1.bar(x + width/2, exe_accs, width, label='Exe_Acc', color='#3498db')\n",
    "    \n",
    "    ax1.set_xlabel('Experiment')\n",
    "    ax1.set_ylabel('Percentage (%)')\n",
    "    ax1.set_title('Execution Rate vs Execution Accuracy')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(exp_names, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 105)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.1f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.1f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Scatter plot: Exe_Rate vs Exe_Acc relationship\n",
    "    ax2 = axes[1]\n",
    "    ax2.scatter(exe_rates, exe_accs, s=100, alpha=0.7, c='#e74c3c')\n",
    "    \n",
    "    for i, name in enumerate(exp_names):\n",
    "        ax2.annotate(name, (exe_rates[i], exe_accs[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax2.set_xlabel('Execution Rate (%)')\n",
    "    ax2.set_ylabel('Execution Accuracy (%)')\n",
    "    ax2.set_title('Exe_Rate vs Exe_Acc Relationship')\n",
    "    ax2.set_xlim(0, 105)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    \n",
    "    # Add diagonal reference line\n",
    "    ax2.plot([0, 100], [0, 100], 'k--', alpha=0.3, label='y=x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate plot\n",
    "plot_execution_metrics(experiments, f\"{CONFIG['output_dir']}/figures/execution_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Performance by Reasoning Depth\n",
    "\n",
    "Following LOGIC-LM Figure 3: Show robustness as complexity increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_reasoning_depth(df: pd.DataFrame, depth_column: str = 'num_refinement_iterations') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze accuracy by reasoning depth/complexity.\n",
    "    \n",
    "    Note: Adapt depth_column based on your data structure.\n",
    "    Could be reasoning_depth, num_hops, problem_complexity, etc.\n",
    "    \"\"\"\n",
    "    if depth_column not in df.columns:\n",
    "        print(f\"Column '{depth_column}' not found. Available columns: {df.columns.tolist()}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    depth_analysis = df.groupby(depth_column).agg(\n",
    "        accuracy=('is_correct', 'mean'),\n",
    "        count=('is_correct', 'count'),\n",
    "        correct=('is_correct', 'sum')\n",
    "    ).reset_index()\n",
    "    \n",
    "    depth_analysis['accuracy'] *= 100\n",
    "    \n",
    "    return depth_analysis\n",
    "\n",
    "\n",
    "def plot_accuracy_by_depth(experiments: Dict[str, pd.DataFrame], \n",
    "                           depth_column: str = 'axiom',\n",
    "                           save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Plot accuracy curves by reasoning depth (Figure 3 style).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(experiments)))\n",
    "    \n",
    "    for (exp_name, df), color in zip(experiments.items(), colors):\n",
    "        if depth_column in df.columns:\n",
    "            depth_data = df.groupby(depth_column)['is_correct'].mean() * 100\n",
    "            \n",
    "            # Plot\n",
    "            ax.plot(range(len(depth_data)), depth_data.values, \n",
    "                   marker='o', linewidth=2, markersize=8,\n",
    "                   label=exp_name.replace('.json', ''), color=color)\n",
    "            \n",
    "            # Set x-tick labels\n",
    "            ax.set_xticks(range(len(depth_data)))\n",
    "            ax.set_xticklabels(depth_data.index, rotation=45, ha='right')\n",
    "    \n",
    "    ax.set_xlabel(depth_column.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'Accuracy by {depth_column.replace(\"_\", \" \").title()}')\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by axiom/rule type\n",
    "for exp_name, df in experiments.items():\n",
    "    print(f\"\\n{exp_name}:\")\n",
    "    \n",
    "    # By axiom\n",
    "    if 'axiom' in df.columns:\n",
    "        axiom_analysis = df.groupby('axiom').agg(\n",
    "            accuracy=('is_correct', lambda x: x.mean() * 100),\n",
    "            count=('is_correct', 'count')\n",
    "        ).round(2)\n",
    "        print(\"\\nAccuracy by Axiom:\")\n",
    "        display(axiom_analysis)\n",
    "    \n",
    "    # By rule_type\n",
    "    if 'rule_type' in df.columns:\n",
    "        rule_analysis = df.groupby('rule_type').agg(\n",
    "            accuracy=('is_correct', lambda x: x.mean() * 100),\n",
    "            count=('is_correct', 'count')\n",
    "        ).round(2)\n",
    "        print(\"\\nAccuracy by Rule Type:\")\n",
    "        display(rule_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy by axiom\n",
    "plot_accuracy_by_depth(experiments, 'axiom', f\"{CONFIG['output_dir']}/figures/accuracy_by_axiom.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Self-Refinement Analysis\n",
    "\n",
    "Following LOGIC-LM Figure 4: Impact of refinement rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_refinement_impact(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze how self-refinement iterations affect performance.\n",
    "    \"\"\"\n",
    "    if 'num_refinement_iterations' not in df.columns:\n",
    "        print(\"No refinement data available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    refinement_analysis = df.groupby('num_refinement_iterations').agg(\n",
    "        accuracy=('is_correct', lambda x: x.mean() * 100),\n",
    "        exe_rate=('execution_success', lambda x: x.mean() * 100),\n",
    "        count=('is_correct', 'count'),\n",
    "        avg_time=('total_time', 'mean'),\n",
    "        avg_llm_calls=('total_llm_calls', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    return refinement_analysis\n",
    "\n",
    "\n",
    "def plot_refinement_analysis(experiments: Dict[str, pd.DataFrame], save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Plot refinement impact (Figure 4 style from LOGIC-LM).\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    for exp_name, df in experiments.items():\n",
    "        ref_data = analyze_refinement_impact(df)\n",
    "        \n",
    "        if ref_data.empty:\n",
    "            continue\n",
    "        \n",
    "        label = exp_name.replace('.json', '')\n",
    "        \n",
    "        # Accuracy by refinement rounds\n",
    "        axes[0].plot(ref_data['num_refinement_iterations'], ref_data['accuracy'],\n",
    "                    marker='o', linewidth=2, markersize=8, label=label)\n",
    "        \n",
    "        # Execution rate by refinement rounds\n",
    "        axes[1].plot(ref_data['num_refinement_iterations'], ref_data['exe_rate'],\n",
    "                    marker='s', linewidth=2, markersize=8, label=label)\n",
    "    \n",
    "    axes[0].set_xlabel('Refinement Iterations')\n",
    "    axes[0].set_ylabel('Accuracy (%)')\n",
    "    axes[0].set_title('Accuracy vs Refinement Rounds')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_xlabel('Refinement Iterations')\n",
    "    axes[1].set_ylabel('Execution Rate (%)')\n",
    "    axes[1].set_title('Exe_Rate vs Refinement Rounds')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refinement analysis\n",
    "for exp_name, df in experiments.items():\n",
    "    print(f\"\\n{exp_name} - Refinement Analysis:\")\n",
    "    ref_data = analyze_refinement_impact(df)\n",
    "    if not ref_data.empty:\n",
    "        display(ref_data)\n",
    "\n",
    "# Plot\n",
    "plot_refinement_analysis(experiments, f\"{CONFIG['output_dir']}/figures/refinement_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Per-Rule/Axiom Breakdown\n",
    "\n",
    "Detailed performance analysis by logical rule type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_per_axiom_heatmap(experiments: Dict[str, pd.DataFrame], save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing accuracy by axiom across experiments.\n",
    "    \"\"\"\n",
    "    # Collect accuracy by axiom for each experiment\n",
    "    axiom_data = {}\n",
    "    \n",
    "    for exp_name, df in experiments.items():\n",
    "        if 'axiom' in df.columns:\n",
    "            axiom_acc = df.groupby('axiom')['is_correct'].mean() * 100\n",
    "            axiom_data[exp_name.replace('.json', '')] = axiom_acc\n",
    "    \n",
    "    if not axiom_data:\n",
    "        print(\"No axiom data available\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame\n",
    "    heatmap_df = pd.DataFrame(axiom_data)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    sns.heatmap(heatmap_df, annot=True, fmt='.1f', cmap='RdYlGn',\n",
    "                vmin=0, vmax=100, ax=ax, cbar_kws={'label': 'Accuracy (%)'})\n",
    "    \n",
    "    ax.set_title('Accuracy by Axiom Across Experiments')\n",
    "    ax.set_xlabel('Experiment')\n",
    "    ax.set_ylabel('Axiom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return heatmap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap\n",
    "heatmap_df = create_per_axiom_heatmap(experiments, f\"{CONFIG['output_dir']}/figures/axiom_heatmap.png\")\n",
    "\n",
    "if heatmap_df is not None:\n",
    "    print(\"\\nAxiom Accuracy Table:\")\n",
    "    display(heatmap_df.round(2))\n",
    "    heatmap_df.to_csv(f\"{CONFIG['output_dir']}/tables/axiom_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Error Analysis\n",
    "\n",
    "Categorize and analyze failure modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_errors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Categorize errors into types for analysis.\n",
    "    \n",
    "    Error categories:\n",
    "    1. Formalization failure - couldn't generate valid logical form\n",
    "    2. Execution failure - valid form but solver failed\n",
    "    3. Semantic error - executed but wrong answer\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    def categorize(row):\n",
    "        if row['is_correct']:\n",
    "            return 'Correct'\n",
    "        elif not row['formalization_success']:\n",
    "            return 'Formalization Error'\n",
    "        elif not row['execution_success']:\n",
    "            return 'Execution Error'\n",
    "        else:\n",
    "            return 'Semantic Error'\n",
    "    \n",
    "    df['error_category'] = df.apply(categorize, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_error_distribution(experiments: Dict[str, pd.DataFrame], save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Plot error distribution across experiments.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(experiments), figsize=(6*len(experiments), 5))\n",
    "    \n",
    "    if len(experiments) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    colors = {'Correct': '#2ecc71', 'Formalization Error': '#e74c3c', \n",
    "              'Execution Error': '#f39c12', 'Semantic Error': '#9b59b6'}\n",
    "    \n",
    "    for ax, (exp_name, df) in zip(axes, experiments.items()):\n",
    "        df_cat = categorize_errors(df)\n",
    "        error_counts = df_cat['error_category'].value_counts()\n",
    "        \n",
    "        wedges, texts, autotexts = ax.pie(\n",
    "            error_counts.values, \n",
    "            labels=error_counts.index,\n",
    "            autopct='%1.1f%%',\n",
    "            colors=[colors.get(cat, '#95a5a6') for cat in error_counts.index],\n",
    "            explode=[0.05 if cat != 'Correct' else 0 for cat in error_counts.index]\n",
    "        )\n",
    "        \n",
    "        ax.set_title(exp_name.replace('.json', ''))\n",
    "    \n",
    "    plt.suptitle('Error Distribution by Category', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "for exp_name, df in experiments.items():\n",
    "    df_cat = categorize_errors(df)\n",
    "    \n",
    "    print(f\"\\n{exp_name} - Error Distribution:\")\n",
    "    error_dist = df_cat['error_category'].value_counts()\n",
    "    error_pct = df_cat['error_category'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    error_summary = pd.DataFrame({\n",
    "        'Count': error_dist,\n",
    "        'Percentage': error_pct.round(2)\n",
    "    })\n",
    "    display(error_summary)\n",
    "\n",
    "# Plot\n",
    "plot_error_distribution(experiments, f\"{CONFIG['output_dir']}/figures/error_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_examples(df: pd.DataFrame, n_examples: int = 3):\n",
    "    \"\"\"\n",
    "    Show example errors for qualitative analysis.\n",
    "    \"\"\"\n",
    "    df_cat = categorize_errors(df)\n",
    "    \n",
    "    for category in ['Formalization Error', 'Execution Error', 'Semantic Error']:\n",
    "        errors = df_cat[df_cat['error_category'] == category]\n",
    "        \n",
    "        if len(errors) > 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Category: {category} ({len(errors)} total)\")\n",
    "            print('='*60)\n",
    "            \n",
    "            for idx, row in errors.head(n_examples).iterrows():\n",
    "                print(f\"\\nExample ID: {row['example_id']}\")\n",
    "                print(f\"Axiom: {row['axiom']}\")\n",
    "                print(f\"Ground Truth: {row['ground_truth']}\")\n",
    "                print(f\"Predicted: {row['converted_answer']}\")\n",
    "                if row['error']:\n",
    "                    print(f\"Error: {row['error'][:200]}...\" if len(str(row['error'])) > 200 else f\"Error: {row['error']}\")\n",
    "                print('-'*40)\n",
    "\n",
    "# Show error examples\n",
    "if experiments:\n",
    "    sample_df = list(experiments.values())[0]\n",
    "    show_error_examples(sample_df, n_examples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Time & Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_efficiency(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Analyze computational efficiency metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'avg_total_time': df['total_time'].mean(),\n",
    "        'std_total_time': df['total_time'].std(),\n",
    "        'avg_formalization_time': df['formalization_time'].mean(),\n",
    "        'avg_refinement_time': df['refinement_time'].mean(),\n",
    "        'avg_solving_time': df['solving_time'].mean(),\n",
    "        'avg_llm_calls': df['total_llm_calls'].mean(),\n",
    "        'total_llm_calls': df['total_llm_calls'].sum()\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_time_breakdown(experiments: Dict[str, pd.DataFrame], save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Plot time breakdown by component.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Stacked bar chart for time breakdown\n",
    "    exp_names = []\n",
    "    form_times = []\n",
    "    ref_times = []\n",
    "    solve_times = []\n",
    "    \n",
    "    for exp_name, df in experiments.items():\n",
    "        exp_names.append(exp_name.replace('.json', ''))\n",
    "        form_times.append(df['formalization_time'].mean())\n",
    "        ref_times.append(df['refinement_time'].mean())\n",
    "        solve_times.append(df['solving_time'].mean())\n",
    "    \n",
    "    x = np.arange(len(exp_names))\n",
    "    width = 0.5\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    ax1.bar(x, form_times, width, label='Formalization', color='#3498db')\n",
    "    ax1.bar(x, ref_times, width, bottom=form_times, label='Refinement', color='#e74c3c')\n",
    "    ax1.bar(x, solve_times, width, bottom=np.array(form_times)+np.array(ref_times), \n",
    "           label='Solving', color='#2ecc71')\n",
    "    \n",
    "    ax1.set_xlabel('Experiment')\n",
    "    ax1.set_ylabel('Average Time (seconds)')\n",
    "    ax1.set_title('Time Breakdown by Component')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(exp_names, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Distribution of total time\n",
    "    ax2 = axes[1]\n",
    "    for exp_name, df in experiments.items():\n",
    "        ax2.hist(df['total_time'], bins=20, alpha=0.5, \n",
    "                label=exp_name.replace('.json', ''), edgecolor='black')\n",
    "    \n",
    "    ax2.set_xlabel('Total Time (seconds)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Processing Time')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency analysis\n",
    "efficiency_table = []\n",
    "\n",
    "for exp_name, df in experiments.items():\n",
    "    metrics = analyze_efficiency(df)\n",
    "    metrics['Experiment'] = exp_name\n",
    "    efficiency_table.append(metrics)\n",
    "\n",
    "efficiency_df = pd.DataFrame(efficiency_table)\n",
    "print(\"Efficiency Metrics:\")\n",
    "display(efficiency_df.round(3))\n",
    "\n",
    "# Save\n",
    "efficiency_df.to_csv(f\"{CONFIG['output_dir']}/tables/efficiency_metrics.csv\", index=False)\n",
    "\n",
    "# Plot\n",
    "plot_time_breakdown(experiments, f\"{CONFIG['output_dir']}/figures/time_breakdown.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_interval(data: np.ndarray, confidence: float = 0.95) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for accuracy.\n",
    "    \"\"\"\n",
    "    n_bootstrap = 10000\n",
    "    bootstrap_means = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        bootstrap_means.append(sample.mean())\n",
    "    \n",
    "    lower = np.percentile(bootstrap_means, (1 - confidence) / 2 * 100)\n",
    "    upper = np.percentile(bootstrap_means, (1 + confidence) / 2 * 100)\n",
    "    \n",
    "    return lower * 100, upper * 100\n",
    "\n",
    "\n",
    "def paired_significance_test(df1: pd.DataFrame, df2: pd.DataFrame, \n",
    "                             metric: str = 'is_correct') -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Perform paired statistical tests between two experiments.\n",
    "    \n",
    "    Uses both paired t-test and Wilcoxon signed-rank test.\n",
    "    \"\"\"\n",
    "    # Align by example_id\n",
    "    merged = pd.merge(df1[['example_id', metric]], df2[['example_id', metric]], \n",
    "                     on='example_id', suffixes=('_1', '_2'))\n",
    "    \n",
    "    if len(merged) == 0:\n",
    "        return {'error': 'No matching examples'}\n",
    "    \n",
    "    vals1 = merged[f'{metric}_1'].astype(float).values\n",
    "    vals2 = merged[f'{metric}_2'].astype(float).values\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, t_pval = ttest_rel(vals1, vals2)\n",
    "    \n",
    "    # Wilcoxon signed-rank test (non-parametric)\n",
    "    try:\n",
    "        w_stat, w_pval = wilcoxon(vals1, vals2)\n",
    "    except ValueError:\n",
    "        w_stat, w_pval = np.nan, np.nan\n",
    "    \n",
    "    return {\n",
    "        'n_paired': len(merged),\n",
    "        'mean_diff': (vals1 - vals2).mean() * 100,\n",
    "        't_statistic': t_stat,\n",
    "        't_pvalue': t_pval,\n",
    "        'wilcoxon_statistic': w_stat,\n",
    "        'wilcoxon_pvalue': w_pval,\n",
    "        'significant_005': t_pval < 0.05,\n",
    "        'significant_001': t_pval < 0.01\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence intervals\n",
    "print(\"95% Confidence Intervals for Accuracy:\\n\")\n",
    "\n",
    "ci_table = []\n",
    "for exp_name, df in experiments.items():\n",
    "    accuracy = df['is_correct'].mean() * 100\n",
    "    ci_lower, ci_upper = compute_confidence_interval(df['is_correct'].values)\n",
    "    \n",
    "    ci_table.append({\n",
    "        'Experiment': exp_name,\n",
    "        'Accuracy (%)': f\"{accuracy:.2f}\",\n",
    "        '95% CI Lower': f\"{ci_lower:.2f}\",\n",
    "        '95% CI Upper': f\"{ci_upper:.2f}\",\n",
    "        'CI Width': f\"{ci_upper - ci_lower:.2f}\"\n",
    "    })\n",
    "\n",
    "ci_df = pd.DataFrame(ci_table)\n",
    "display(ci_df)\n",
    "ci_df.to_csv(f\"{CONFIG['output_dir']}/tables/confidence_intervals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired significance tests (if multiple experiments)\n",
    "if len(experiments) >= 2:\n",
    "    exp_list = list(experiments.items())\n",
    "    \n",
    "    print(\"\\nPaired Significance Tests:\\n\")\n",
    "    \n",
    "    for i in range(len(exp_list)):\n",
    "        for j in range(i+1, len(exp_list)):\n",
    "            name1, df1 = exp_list[i]\n",
    "            name2, df2 = exp_list[j]\n",
    "            \n",
    "            results = paired_significance_test(df1, df2)\n",
    "            \n",
    "            print(f\"{name1} vs {name2}:\")\n",
    "            print(f\"  N pairs: {results.get('n_paired', 'N/A')}\")\n",
    "            print(f\"  Mean difference: {results.get('mean_diff', 'N/A'):.2f}%\")\n",
    "            print(f\"  t-test p-value: {results.get('t_pvalue', 'N/A'):.4f}\")\n",
    "            print(f\"  Significant (p<0.05): {results.get('significant_005', 'N/A')}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Publication-Ready Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_main_comparison_figure(experiments: Dict[str, pd.DataFrame], \n",
    "                                   save_path: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Create the main comparison figure for the paper.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Create grid\n",
    "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.25)\n",
    "    \n",
    "    # 1. Main accuracy bar chart\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    \n",
    "    exp_names = [name.replace('.json', '').replace('_', ' ') for name in experiments.keys()]\n",
    "    accuracies = [df['is_correct'].mean() * 100 for df in experiments.values()]\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(experiments)))\n",
    "    bars = ax1.bar(exp_names, accuracies, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    ax1.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "    ax1.set_title('(a) Overall Accuracy', fontweight='bold')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.set_xticklabels(exp_names, rotation=45, ha='right')\n",
    "    \n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax1.annotate(f'{acc:.1f}%', \n",
    "                    xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                    xytext=(0, 5), textcoords='offset points',\n",
    "                    ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # 2. Execution metrics\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    x = np.arange(len(experiments))\n",
    "    width = 0.35\n",
    "    \n",
    "    exe_rates = [compute_execution_metrics(df)['exe_rate'] for df in experiments.values()]\n",
    "    exe_accs = [compute_execution_metrics(df)['exe_acc'] for df in experiments.values()]\n",
    "    \n",
    "    bars1 = ax2.bar(x - width/2, exe_rates, width, label='Exe_Rate', color='#27ae60', edgecolor='black')\n",
    "    bars2 = ax2.bar(x + width/2, exe_accs, width, label='Exe_Acc', color='#2980b9', edgecolor='black')\n",
    "    \n",
    "    ax2.set_ylabel('Percentage (%)', fontweight='bold')\n",
    "    ax2.set_title('(b) Execution Analysis', fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(exp_names, rotation=45, ha='right')\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.set_ylim(0, 105)\n",
    "    \n",
    "    # 3. Per-axiom performance\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    sample_df = list(experiments.values())[0]\n",
    "    if 'axiom' in sample_df.columns:\n",
    "        axiom_acc = sample_df.groupby('axiom')['is_correct'].mean() * 100\n",
    "        axiom_acc = axiom_acc.sort_values(ascending=True)\n",
    "        \n",
    "        colors_axiom = plt.cm.RdYlGn(axiom_acc.values / 100)\n",
    "        bars3 = ax3.barh(axiom_acc.index, axiom_acc.values, color=colors_axiom, edgecolor='black')\n",
    "        \n",
    "        ax3.set_xlabel('Accuracy (%)', fontweight='bold')\n",
    "        ax3.set_title('(c) Accuracy by Axiom', fontweight='bold')\n",
    "        ax3.set_xlim(0, 100)\n",
    "    \n",
    "    # 4. Error distribution\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    df_cat = categorize_errors(sample_df)\n",
    "    error_counts = df_cat['error_category'].value_counts()\n",
    "    \n",
    "    colors_pie = {'Correct': '#27ae60', 'Formalization Error': '#e74c3c', \n",
    "                  'Execution Error': '#f39c12', 'Semantic Error': '#9b59b6'}\n",
    "    \n",
    "    wedges, texts, autotexts = ax4.pie(\n",
    "        error_counts.values,\n",
    "        labels=error_counts.index,\n",
    "        autopct='%1.1f%%',\n",
    "        colors=[colors_pie.get(cat, '#95a5a6') for cat in error_counts.index],\n",
    "        explode=[0.03] * len(error_counts),\n",
    "        shadow=True\n",
    "    )\n",
    "    ax4.set_title('(d) Error Distribution', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate main figure\n",
    "create_main_comparison_figure(experiments, f\"{CONFIG['output_dir']}/figures/main_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_latex_table(df: pd.DataFrame, caption: str, label: str) -> str:\n",
    "    \"\"\"\n",
    "    Export DataFrame as LaTeX table for paper.\n",
    "    \"\"\"\n",
    "    latex = df.to_latex(index=False, escape=False)\n",
    "    \n",
    "    # Wrap in table environment\n",
    "    full_latex = f\"\"\"\\\\begin{{table}}[t]\n",
    "\\\\centering\n",
    "\\\\caption{{{caption}}}\n",
    "\\\\label{{{label}}}\n",
    "{latex}\\\\end{{table}}\"\"\"\n",
    "    \n",
    "    return full_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LaTeX tables\n",
    "if not main_results.empty:\n",
    "    latex_main = export_latex_table(\n",
    "        main_results, \n",
    "        \"Main results comparing methods across datasets.\",\n",
    "        \"tab:main_results\"\n",
    "    )\n",
    "    \n",
    "    with open(f\"{CONFIG['output_dir']}/tables/main_results.tex\", 'w') as f:\n",
    "        f.write(latex_main)\n",
    "    \n",
    "    print(\"LaTeX table saved to tables/main_results.tex\")\n",
    "    print(\"\\nPreview:\")\n",
    "    print(latex_main[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "def generate_summary_report(experiments: Dict[str, pd.DataFrame]) -> str:\n",
    "    \"\"\"\n",
    "    Generate a text summary of all experiments.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\"*60)\n",
    "    report.append(\"NEURO-SYMBOLIC REASONING EXPERIMENT SUMMARY\")\n",
    "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(\"=\"*60)\n",
    "    \n",
    "    for exp_name, df in experiments.items():\n",
    "        report.append(f\"\\n\\n{'='*40}\")\n",
    "        report.append(f\"Experiment: {exp_name}\")\n",
    "        report.append('='*40)\n",
    "        \n",
    "        # Metadata\n",
    "        metadata = df.attrs.get('metadata', {})\n",
    "        report.append(f\"\\nModel: {metadata.get('model', 'Unknown')}\")\n",
    "        report.append(f\"Logic Type: {metadata.get('logic_type', 'Unknown')}\")\n",
    "        report.append(f\"Total Examples: {len(df)}\")\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = df['is_correct'].mean() * 100\n",
    "        report.append(f\"\\nOverall Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Execution metrics\n",
    "        exe_metrics = compute_execution_metrics(df)\n",
    "        report.append(f\"Execution Rate: {exe_metrics['exe_rate']:.1f}%\")\n",
    "        report.append(f\"Execution Accuracy: {exe_metrics['exe_acc']:.1f}%\")\n",
    "        \n",
    "        # Efficiency\n",
    "        eff_metrics = analyze_efficiency(df)\n",
    "        report.append(f\"\\nAvg Time per Query: {eff_metrics['avg_total_time']:.2f}s\")\n",
    "        report.append(f\"Avg LLM Calls: {eff_metrics['avg_llm_calls']:.1f}\")\n",
    "        \n",
    "        # Per-axiom (if available)\n",
    "        if 'axiom' in df.columns:\n",
    "            report.append(\"\\nPer-Axiom Accuracy:\")\n",
    "            axiom_acc = df.groupby('axiom')['is_correct'].mean() * 100\n",
    "            for axiom, acc in axiom_acc.items():\n",
    "                report.append(f\"  {axiom}: {acc:.1f}%\")\n",
    "    \n",
    "    return '\\n'.join(report)\n",
    "\n",
    "# Generate and save report\n",
    "report = generate_summary_report(experiments)\n",
    "print(report)\n",
    "\n",
    "with open(f\"{CONFIG['output_dir']}/summary_report.txt\", 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nReport saved to {CONFIG['output_dir']}/summary_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary of all outputs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE - OUTPUT FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(CONFIG['output_dir']):\n",
    "    level = root.replace(CONFIG['output_dir'], '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


\section{Appendix}

\subsection{Detailed Implementation: From Text to Logic}
\label{sec:implementation}

This section provides a comprehensive description of our implementation pipeline for converting natural language documents into logified structures. The implementation is organized into four main modules: (1) \texttt{logify.py} orchestrates the overall conversion, (2) \texttt{openie\_extractor.py} performs relation triple extraction, (3) \texttt{logic\_converter.py} performs LLM-based logic conversion, and (4) \texttt{weights.py} assigns confidence weights to constraints. Additionally, the \texttt{interface\_with\_user} module handles query-time translation via \texttt{translate.py}.

\subsubsection{Pipeline Architecture Overview}

The complete pipeline is implemented in Python and follows a modular design:

\begin{verbatim}
Document (PDF/DOCX/TXT)
    |
    v
[LogifyConverter] ---> [OpenIEExtractor] ---> Relation triples
    |                       - Coreference resolution
    |                       - OpenIE extraction
    |                       - Dependency parse fallback
    |
    +---> [LogicConverter] ---> Propositions + constraints (JSON)
    |         - LLM-based conversion
    |         - System prompt with few-shot examples
    |
    +---> [assign_weights] ---> Weighted constraints
              - SBERT retrieval
              - LLM logprob extraction

Query (Natural Language)
    |
    v
[translate_query] ---> Propositional formula
    |
    v
[LogicSolver] ---> TRUE / FALSE / UNCERTAIN + confidence
\end{verbatim}

\subsubsection{Stage 1: OpenIE Extraction}

The \texttt{OpenIEExtractor} class extracts relation triples using three components:

\paragraph{Coreference Resolution.}
We use Stanza's native coreference model to replace pronouns with canonical mentions:
\begin{verbatim}
def _resolve_coreferences(self, text):
    doc = self.coref_pipeline(text)
    for chain in doc.coref_chains:
        representative = chain.representative_text
        for mention in chain:
            if not mention.is_representative:
                # Replace mention with representative
                ...
    return resolved_text
\end{verbatim}

\paragraph{CoreNLP OpenIE.}
After coreference resolution, we extract triples via Stanford CoreNLP:
\begin{verbatim}
self.openie_properties = {
    'openie.triple.strict': 'true',
    'openie.triple.all_nominals': 'true',
    'openie.max_entailments_per_clause': '3',
}
\end{verbatim}

\paragraph{Dependency Parse Fallback.}
When OpenIE fails, we extract subject-verb-object triples from the dependency parse, marked with \texttt{source='stanza\_depparse'}.

\subsubsection{Stage 2: LLM-based Logic Conversion}

The \texttt{LogicConverter} class uses a reasoning model (GPT-5.2) with a structured system prompt. The complete prompt is provided in Appendix~\ref{sec:prompt}. Key design principles:
\begin{itemize}
    \item \textbf{Faithfulness}: Prioritize accurate representation over completeness
    \item \textbf{Atomicity}: Propositions cannot be further decomposed
    \item \textbf{Provenance}: Every element includes textual evidence
\end{itemize}

The LLM receives both the original text and OpenIE triples:
\begin{verbatim}
ORIGINAL TEXT:
<<<{text}>>>

RELATION TRIPLES:
<<<{formatted_triples}>>>
\end{verbatim}

\subsubsection{Stage 3: Weight Assignment}
\label{sec:weight_assignment}

The \texttt{assign\_weights} function assigns confidence weights $w(Q) \in (0,1)$ to constraints using SBERT retrieval and LLM logprob extraction:

\begin{enumerate}
    \item Chunk document and compute SBERT embeddings
    \item For each constraint $Q$, retrieve top-$k$ relevant chunks
    \item Prompt LLM with chunks asking ``Does text support $Q$?''
    \item Extract YES/NO logprobs: $(\ell_{\text{yes}}, \ell_{\text{no}})$
    \item Compute softmax confidence: $d(p) = \frac{\exp(\ell_{\text{yes}})}{\exp(\ell_{\text{yes}}) + \exp(\ell_{\text{no}})}$
    \item Also verify negated constraint; compute final weight via binary softmax
\end{enumerate}

Interpretation: $w(Q) \approx 1$ indicates strong support; $w(Q) \approx 0.5$ indicates neutral/mixed evidence.

\subsubsection{Query Translation}

At query time, \texttt{translate\_query} converts natural language to propositional formulas:

\begin{enumerate}
    \item Detect Yes/No questions and convert to declarative statements
    \item Retrieve top-$k$ propositions via SBERT similarity
    \item Prompt LLM to translate hypothesis to formula over retrieved propositions
    \item Validate formula against proposition schema
\end{enumerate}

\subsubsection{Logic Solver}

The \texttt{LogicSolver} class encodes the logified structure as Weighted CNF and uses PySAT's RC2 MaxSAT solver \cite{ignatiev2019rc2}:

\begin{itemize}
    \item \textbf{Entailment}: Check if $\text{KB} \land \neg q$ is UNSAT
    \item \textbf{Consistency}: Check if $\text{KB} \land q$ is SAT
    \item \textbf{Confidence}: Compare MaxSAT costs for $q$ vs $\neg q$
\end{itemize}

Soft constraints use log-odds weighting: $\text{weight}(C_k) = \frac{w_k}{1 - w_k} \times 1000$.

\subsubsection{Usage}
\label{sec:usage}

\begin{verbatim}
# Logify mode: create structure from document
python main.py logify --fpath "document.pdf"

# Query mode: ask questions
python main.py query --fpath "doc.pdf" --query "Is X true?"
\end{verbatim}

\subsubsection{Configuration Defaults}

\begin{table}[h]
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Default} & \textbf{Description} \\
\midrule
\texttt{model} & gpt-5.2 & Reasoning model for logic conversion \\
\texttt{reasoning\_effort} & medium & Reasoning depth \\
\texttt{weights\_model} & gpt-4o & Model for verification (needs logprobs) \\
\texttt{k\_weights} & 10 & Top-k chunks for weight assignment \\
\texttt{k\_query} & 20 & Top-k propositions for query \\
\texttt{sbert\_model} & all-MiniLM-L6-v2 & SBERT model for retrieval \\
\bottomrule
\end{tabular}
\caption{Default configuration parameters.}
\label{tab:config_defaults}
\end{table}


\subsection{System Prompt for Logic Conversion}
\label{sec:prompt}

The following is the complete system prompt provided to the LLM for logic conversion:

\begin{quote}
\small
\textbf{ROLE:} You are a neuro-symbolic reasoning assistant and expert logician that translates natural language text into structured zeroth-order propositional logic, with the assistance of preprocessed OpenIE relation triples.

\textbf{TASK:} Given a natural language text $T$ and its corresponding OpenIE relation triples, extract:

\begin{enumerate}
    \item \textbf{Atomic, primitive propositions} denoted $P_1, P_2, \ldots, P_n$. Each primitive propositional variable should be independent, contain no logical connectives, and be equipped with its natural language meaning along with short evidence from the text.

    \item \textbf{Hard constraints} which are zeroth-order propositional formulas over the variables $P_1, P_2, \ldots, P_n$ that must hold. These should include their meaning from the text, and evidence for why it is a required constraint.

    \item \textbf{Soft constraints} which are also zeroth-order propositional formulas over the variables $P_1, P_2, \ldots, P_n$ that might hold. These are equipped with their meaning and evidence for why it is a soft constraint. Soft constraints reflect hedged or probabilistic language in the text (e.g., ``typically,'' ``sometimes,'' ``may'').
\end{enumerate}

\textbf{METHODOLOGY:}
\begin{itemize}
    \item \textbf{Step 1:} Analyze OpenIE triples to understand extracted entities and relationships.
    \item \textbf{Step 2:} Define primitive propositions---atomic, truth-evaluable statements.
    \item \textbf{Step 3:} Extract hard constraints---formulas that must hold per the text.
    \item \textbf{Step 4:} Extract soft constraints---formulas that may hold (defeasible statements).
\end{itemize}

\textbf{GUIDELINES:}
\begin{itemize}
    \item Use zeroth-order (propositional) logic only.
    \item Flatten first-order-like expressions: e.g., \texttt{Study(Alice, Tuesday)} becomes ``Alice studies on Tuesday.''
    \item Be faithful to the spirit of the text; do not add artificial primitives or constraints.
    \item When OpenIE triples conflict with the text, prioritize the original text.
\end{itemize}

\textbf{GRAMMAR:} Given a set $P$ of propositional variables, formulas are defined inductively:
\begin{enumerate}
    \item Every propositional variable is a formula.
    \item If $p$ is a formula, then $\neg p$ is a formula.
    \item If $p$ and $q$ are formulas, then $p \land q$, $p \lor q$, $p \Rightarrow q$, and $p \Leftrightarrow q$ are formulas.
\end{enumerate}

\textbf{OUTPUT FORMAT:} Return valid JSON:
\begin{verbatim}
{
  "primitive_props": [
    {"id": "P_1", "translation": "...",
     "evidence": "...", "explanation": "..."}
  ],
  "hard_constraints": [
    {"id": "H_1", "formula": "...",
     "translation": "...", "evidence": "...",
     "reasoning": "..."}
  ],
  "soft_constraints": [
    {"id": "S_1", "formula": "...",
     "translation": "...", "evidence": "...",
     "reasoning": "..."}
  ]
}
\end{verbatim}
\end{quote}

A complete worked example demonstrating this prompt is provided in Appendix~\ref{sec:exemplar}.


\subsection{Extraction Exemplar}
\label{sec:exemplar}

This section provides a complete worked example demonstrating the logic conversion process.

\subsubsection{Input Text}

\begin{quote}
The hospital's emergency triage protocol requires immediate attention for patients presenting with chest pain, unless the pain is clearly musculoskeletal in origin and the patient is under 40 years old. Dr.\ Martinez, who has been working double shifts this week, believes that patients over 65 should always receive an ECG regardless of symptoms, although Dr.\ Yang only sometimes believes this. The official guidelines only mandate this when cardiac history is documented.
\end{quote}

\subsubsection{Expected Output}

\paragraph{Primitive Propositions:}

\begin{itemize}
    \item[$P_1$:] ``The patient requires immediate attention under the hospital's emergency triage protocol.''
    \item[$P_2$:] ``The patient presents with chest pain that is not clearly musculoskeletal in origin.''
    \item[$P_3$:] ``The patient presents with chest pain that is clearly musculoskeletal in origin.''
    \item[$P_4$:] ``The patient is under 40 years old.''
    \item[$P_5$:] ``Dr.\ Martinez works double shifts this week.''
    \item[$P_6$:] ``The patient is over 65 years old.''
    \item[$P_7$:] ``Dr.\ Martinez makes sure the patient receives an ECG.''
    \item[$P_8$:] ``Dr.\ Martinez follows the official guidelines.''
    \item[$P_9$:] ``Dr.\ Yang follows the official guidelines.''
    \item[$P_{10}$:] ``Dr.\ Yang makes sure the patient receives an ECG.''
    \item[$P_{11}$:] ``The patient has a documented cardiac history.''
\end{itemize}

\paragraph{Hard Constraints:}

\begin{itemize}
    \item[$H_1$:] $(P_2 \land \neg(P_3 \land P_4)) \Rightarrow P_1$ \\
    \textit{Reasoning:} ``Unless'' indicates exception; the patient requires attention if chest pain is present and the exception doesn't apply.

    \item[$H_2$:] $P_5$ \\
    \textit{Reasoning:} Stated as fact in the text.

    \item[$H_3$:] $P_6 \Rightarrow P_7$ \\
    \textit{Reasoning:} ``Always'' indicates hard constraint for Dr.\ Martinez's actions.

    \item[$H_4$:] $((P_8 \land P_{11}) \Rightarrow P_7) \land ((P_9 \land P_{11}) \Rightarrow P_{10})$ \\
    \textit{Reasoning:} Guidelines apply to all doctors; flattened to propositional logic by instantiating for each doctor.
\end{itemize}

\paragraph{Soft Constraints:}

\begin{itemize}
    \item[$S_1$:] $P_6 \Rightarrow P_{10}$ \\
    \textit{Reasoning:} ``Sometimes'' indicates soft constraint for Dr.\ Yang's actions.
\end{itemize}

\subsubsection{Key Observations}

\begin{enumerate}
    \item \textbf{Linguistic cues}: ``Always'' yields hard constraint; ``sometimes'' yields soft constraint for the same logical structure.
    \item \textbf{Flattening quantification}: Universal guidelines are instantiated per-entity.
    \item \textbf{Preserving structure}: ``Unless'' is represented as $\neg(P_3 \land P_4)$ rather than equivalent $\neg P_3 \lor \neg P_4$.
\end{enumerate}


\subsection{Propositional vs.\ First-Order Logic}

\begin{table}[h!]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{Logic-LM Exec} & \textbf{Logify Exec} \\
\midrule
ProofWriter (prop-like) & 99.0 & 99.4 \\
FOLIO (FOL) & 85.8 & 91.2 \\
\bottomrule
\end{tabular}
\caption{Execution rates. Propositional-like tasks achieve near-perfect translation.}
\label{tab:propositional_vs_FOL}
\end{table}


\subsection{Worked Example: START and Jump-START Triage}
\label{sec:start_example}

We illustrate our constructions on the following text $T$:

\begin{quote}
\emph{``The START triage system is widely used in the United States;
it is utilized for patients above age $8$; it is intended that triage status can be computed in under $60$ seconds.
For children, a commonly used algorithm is Jump-START, which is based on START but accounts for pediatric respiratory failure risk
and difficulty following verbal commands. Triage is a dynamic process, and a patient can change triage status over time.''}
\end{quote}

\subsubsection{Extraction}

Atomic propositions $\mathcal{P}(T)=\{P_1,\dots,P_9\}$:
\begin{itemize}
\item $P_1$: ``START is widely used in the US.''
\item $P_2$: ``START is for patients above age 8.''
\item $P_3$: ``Triage status intended in under 60 seconds.''
\item $P_4$: ``Jump-START is commonly used for children.''
\item $P_5$: ``Jump-START is based on START.''
\item $P_6$: ``Jump-START accounts for pediatric respiratory failure risk.''
\item $P_7$: ``Children may be unable to follow verbal commands.''
\item $P_8$: ``Triage is a dynamic process.''
\item $P_9$: ``A patient can change triage status over time.''
\end{itemize}

Hard constraints $\mathcal{C}(T)$: $P_8 \rightarrow P_9$, $P_4 \rightarrow P_5$, $P_4 \rightarrow P_6$, $P_4 \rightarrow P_7$.

Soft constraints with weights: $S_1 := P_1$ ($w=0.75$), $S_2 := P_2$ ($w=0.60$), $S_3 := P_3$ ($w=0.85$).

\subsubsection{Knowledge Algebra}

The Boolean ring $\mathcal{R}(T)$ enforces hard constraints via the ideal:
\[
\mathcal{I}_{\mathcal{C}}(T) = \langle P_8(1+P_9), P_4(1+P_5), P_4(1+P_6), P_4(1+P_7) \rangle.
\]

Readings are weighted by soft constraints. For reading $\varphi_A$ satisfying all soft constraints:
$W(\varphi_A) = (0.75)(0.60)(0.85) = 0.3825$.
For reading $\varphi_B$ violating $P_1, P_3$:
$W(\varphi_B) = (0.25)(0.60)(0.15) = 0.0225$.
Reading A is preferred by factor $W(\varphi_A)/W(\varphi_B) = 17$.


\subsection{Algebraic Reasoning: Consequence and Compatibility}
\label{sec:algebraic_reasoning}

\begin{proposition}[Consequence and Inconsistency]
\label{prop:consequence_inconsistency}
Let $q := \Pi_{\mathrm{form}}(Q) \in \mathcal{R}(T)$ encode a query $Q$.
\begin{enumerate}
\item $Q$ is a \emph{$\Pi$-consequence} of $T$ iff $1+q \in \mathcal{I}_{\mathcal{C}}(T)$.
\item $Q$ is \emph{$\Pi$-inconsistent} with $T$ iff $q \in \mathcal{I}_{\mathcal{C}}(T)$.
\end{enumerate}
\end{proposition}

\begin{proposition}[Compatibility]
\label{prop:compatibility}
Propositions $S$ and $R$ are \emph{mutually incompatible} iff $sr \in \mathcal{I}_{\mathcal{C}}(T)$.
\end{proposition}

\begin{definition}[Underdetermination]
\label{def:underdetermination}
The underdetermination ratio is $\mathrm{Und}(T,\Pi) := \frac{\log_2 N(T)}{n}$, where $N(T) = |\mathrm{Hom}(\mathcal{R}(T), \mathbb{F}_2)|$ is the number of readings and $n$ is the number of atoms.
\end{definition}


\subsection{Baseline Choices}
\label{sec:baselines}

\paragraph{Short-form tasks (FOLIO, ProofWriter).}
\begin{itemize}
    \item \textbf{Logic-LM} \cite{pan2023logiclm}: Per-query symbolic formalization
    \item \textbf{Reasoning LLM}: Chain-of-thought without symbolic delegation
\end{itemize}

\paragraph{Long-form tasks (ContractNLI).}
\begin{itemize}
    \item \textbf{Logic-LM + RAG}: Per-query formalization over retrieved passages
    \item \textbf{Reasoning LLM + RAG}: Retrieval-augmented reasoning
\end{itemize}


\subsection{Logic-LM++ Baseline Configuration}
\label{sec:logiclm_baseline}

We evaluate Logic-LM++ on LogicBench-v1.0 modus tollens tasks using GPT-4 with propositional (not FOL) formalization.

\paragraph{Configuration.}
Refinement: min 1, max 4 iterations with backtracking agent. Solver: Z3 with 30s timeout.

\paragraph{Results.}
50\% accuracy (10/20), 100\% execution rate. All examples triggered REVERT decisions, indicating refinements did not improve over initial formalization.

\begin{table}[h]
\centering
\small
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Overall Accuracy & 50.0\% \\
Execution Rate & 100.0\% \\
Avg.\ Time per Query & 10.94 s \\
Backtracking Rate & 100\% REVERT \\
\bottomrule
\end{tabular}
\caption{Logic-LM++ results on modus tollens.}
\label{tab:logiclm-summary}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Question Type} & \textbf{Count} & \textbf{Correct} & \textbf{Accuracy} \\
\midrule
Positive query & 10 & 2 & 20.0\% \\
Negative query & 10 & 8 & 80.0\% \\
\bottomrule
\end{tabular}
\caption{Accuracy by question polarity.}
\label{tab:logiclm-polarity}
\end{table}

\paragraph{Key Findings.}
(1) Solving takes $<0.1\%$ of time; formalization is the bottleneck.
(2) Negative queries achieve 80\% vs.\ 20\% for positive (modus tollens naturally concludes $\neg P$).
(3) Primary failure: predicate misalignment (introducing $R$ instead of recognizing $\neg Q$).
(4) The 50\% ceiling highlights limitations of per-query formalization that Logify's ``logify once'' paradigm addresses.


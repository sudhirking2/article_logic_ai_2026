\begin{abstract}
We propose a neuro-symbolic framework for logical reasoning over long-form documents (e.g., contracts, specifications) relative to an explicit extraction pipeline. Rather than retrieving passages at query time, we compile the document once into a logified representation: atomic propositions with hard (mandatory) constraints and weighted soft constraints that rank plausible readings. This "logify once, query many" paradigm supports repeated reasoning via SAT-based solving and a Boolean-algebraic formalization, without re-processing the raw text. On long-document benchmarks with non-local constraints, we improve over RAG baselines while providing solver-checkable outputs and conditional correctness guarantees given the extracted theory.
\end{abstract}

\section{Introduction}

Reliable reasoning over text remains a core challenge in AI.
Given a document---a contract, medical guideline, or technical specification---users need to determine whether a query is entailed, contradicted, or underdetermined by the text.
Large language models (LLMs) provide natural-language interfaces for such tasks, but they conflate two fundamentally different problems: \emph{language semantics} (extracting meaning from text) and \emph{logical reasoning} (deriving valid conclusions from premises stated in the text).
This conflation is the source of their unreliability.
LLMs hallucinate facts, preferring external knowledge to the input document~\citep{ji2023hallucination}; miss information in long contexts, prioritizing the start and end while neglecting the middle~\citep{liu2024lost}; and produce plausible but logically unsound conclusions~\citep{huang2023reasoning}.

A recent survey identifies neuro-symbolic methods, which separate language semantics from logical reasoning, as the most effective paradigm for improving reasoning in LLMs~\citep{yang2025neurosymbolic}. One class of neuro-symbolic methods translates natural language into formal representations, delegates inference to symbolic solvers, and translates results back---the LLM serves as a bridge between natural language and formal logic. However, these approaches have been evaluated on short reasoning problems—the standard benchmarks (FOLIO, ProofWriter, PrOntoQA, AR-LSAT, LogicalDeduction, ReClor) contain examples of a few hundred tokens. Even if retrieval-augmented generation (RAG) is employed to handle longer documents, it may fail to capture logical constraints scattered across the text. For example, a non-disclosure clause on page 3 may interact with an exception on page 12, and retrieval cannot guarantee that all relevant pieces are brought into context.

To handle large documents, we propose a different approach: \emph{logify} the document once, then reason symbolically over the resulting structure. Our pipeline extracts atomic propositions, hard constraints (definite statements that must hold), and soft constraints (defeasible statements with weights measuring uncertainty). This structure supports faithful reasoning---entailment, satisfiability, uncertainty quantification---via a weighted Max-SAT solver.

\paragraph{Contributions.}
\begin{enumerate}
    \item A system architecture that logifies text once and supports repeated
    querying via weighted Max-SAT (Section~\ref{sec:system}).
    \item Experiments on logical reasoning and document-level NLI benchmarks,
    demonstrating consistent gains over RAG and neuro-symbolic baselines, with
    the largest improvements on long documents (Section~\ref{sec:experiments}).
    \item An algebraic framework for representing the finite logical theory
    with hard and soft constraints as an algebraic structure (Section~\ref{sec:framework}).
\end{enumerate}

\subsection{Related Work}
\label{sec:related_work}

\paragraph{Neuro-symbolic logical reasoning.}
Recent work combines LLMs with symbolic solvers to improve logical reasoning.
Logic-LM++ \citep{parmar2024logiclmpp} extends Logic-LM \citep{pan2023logiclm} with
multi-step refinement, achieving 78.9\% on FOLIO (204 examples, $\sim$75 tokens each)
and 79.7\% on ProofWriter (600 examples, $\sim$100 tokens each).
LINC \citep{olausson2023linc} uses first-order logic with Prover9, reaching
87\% on ProofWriter with GPT-4. Both systems formalize problems \emph{per query}
on short benchmarks; in contrast, our approach logifies the document once and supports
repeated querying over long texts. Moreover, these systems treat all constraints as hard;
we introduce soft constraints with confidence weights, enabling reasoning under uncertainty.

\paragraph{Tool-augmented LLMs.}
A broader line of work augments LLMs using external tools for faithful computation.
PAL \citep{gao2023pal} and Program-of-Thought \citep{chen2023pot} use code execution
for arithmetic reasoning. SatLM \citep{ye2023satlm} targets satisfiability problems.
Logic-of-Thought \citep{liu2024lot} injects propositional logic to generate augmented prompts, and let an LLM reason.
These have been shown to perform better than Chain-of-Thought baselines.

\paragraph{Algebraic and proof-theoretic foundations.}
Our framework builds on algebraic proof systems, where polynomial encodings yield 
certificates of propositional unsatisfiability \cite{CleggEdmondsImpagliazzo1996}. 
We use Boolean polynomial rings, computable via tools like PolyBoRi 
\cite{brickenstein2009polybori}. From the model-theoretic perspective, de Kleer's 
assumption-based truth maintenance system (ATMS) \cite{deKleer1986} compactly 
represents multiple consistent assignments under evolving constraints—conceptually 
parallel to our notion of ring homomorphisms $\phi: R(T) \to \mathbb{F}_2$ 
as a consistent assignment.

\paragraph{Weighted reasoning and model counting.}
When soft constraints carry confidences, we connect to weighted Max-SAT 
\cite{heras2007minimaxsat} and Algebraic Model Counting (AMC) 
\cite{KimmigVanDenBroeckDeRaedt2017}, which 
generalizes weighted model counting to commutative semirings. This provides a 
principled route to ranking readings by plausibility and quantifying underdetermination.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{article/structure.jpeg}
    \caption{The document is processed once through a three-stage pipeline: (1) we extract relational triples from Stanford OpenIE, (2) we pass the text and triples to an LLM (GPT-5.2) once to extract atomic propositions,  hard constraints, and soft constraints, and (3) we assign weights via SBERT retrieval and LLM verification. At query time, the user's question is translated to a propositional formula (using SBERT retrieval over the translations of the atomic propositions), and the MaxSAT solver determines entailment, contradiction, or uncertainty.}
    \label{fig:arch}
\end{figure}

\section{System Architecture}
\label{sec:system}

Our architecture follows a modular design with three components: (1) \texttt{from\_text\_to\_logic}, which logifies text into propositions and constraints;  (2) \texttt{interface\_with\_user}, which handles natural language interaction for queries, and (3) \texttt{logic\_solver}, which performs reasoning via weighted Max-SAT. Figure~\ref{fig:arch} illustrates the overall pipeline.



\subsection{From Text to Logic}
\label{sec:from_text_to_logic}

This module implements the extraction of natural language text into the weighted logified structure in three stages.

\subsubsection{Relation Triple Extraction}
We use Stanford CoreNLP's OpenIE system \cite{angeli2015openie} via the Stanza Python interface \cite{qi2020stanza} to extract subject-predicate-object triples from the text. This provides structured relational information that can help guide proposition identification. We first apply native Stanza coreference resolution to handle pronominal references (e.g. replacing "They" with "Alice"), to ensure that triples consistently refer to canonical entities rather than pronouns. As a fallback, we use Stanza's dependency parse for sentences where OpenIE fails to extract triples (e.g., intransitive verbs with adverbs). Triples are formatted as \texttt{[subject, predicate, object]}.

\subsubsection{LLM-Based Logic Conversion}
The original text $T$ and the extracted triples are jointly provided to a large language model (GPT-5.2 by default). The LLM performs semantic analysis to identify atomic propositions $\mathcal{P}(T)$, hard constraints $\mathcal{C}(T)$, and soft constraints $\mathcal{S}(T)$---crucially, soft constraints are extracted \emph{without} weights at this stage.

\subsubsection{Weight Assignment}
The final stage assigns confidence weights to all constraints using an evidence-based retrieval and LLM verification pipeline. For each constraint $Q$, we: (1) chunk the original document and pre-compute SBERT embeddings \cite{reimers2019sbert} (model: \texttt{all-MiniLM-L6-v2}); (2) retrieve the top-$k$ chunks most similar to the constraint's natural language translation ($k=10$ by default); (3) prompt an LLM with the retrieved chunks, asking a YES/NO verification question: ``Does this document excerpt provide clear evidence supporting this claim?''; (4) extract logprobs for YES and NO tokens from the response; (5) repeat for the \emph{negated} constraint: ``It is not the case that [Q]''; and (6) compute confidence via binary softmax: $w(Q) = P(\text{YES} \mid Q) / (P(\text{YES} \mid Q) + P(\text{YES} \mid \neg Q) + \epsilon)$, where $\epsilon$ is a small constant for numerical stability. This produces a weight $w(Q) \in (0,1)$ reflecting how strongly the document supports $Q$. The separation is deliberate: the LLM identifies \emph{which} constraints are defeasible based on linguistic cues, while the weight assignment module quantifies \emph{how strongly} the text supports each constraint. The output is stored as a triplet $[\text{prob\_yes\_orig}, \text{prob\_yes\_neg}, \text{confidence}]$ for transparency, where $\text{confidence}$ is the calibrated score used for downstream reasoning.

\subsubsection{Output: The Logified Structure}
The module outputs a JSON structure with rich provenance metadata. After Stage 3, the weighted structure includes:
\begin{verbatim}
{"primitive_props": [
   {"id": "P_1", "translation": "...",
    "evidence": "...",
    "explanation": "..."}],
 "hard_constraints": [
   {"id": "H_1", "formula": "P_1 -> P_2",
    "translation": "...",
    "evidence": "...",
    "reasoning": "...",
    "weight": [0.95, 0.12, 0.89]}],
 "soft_constraints": [
   {"id": "S_1", "formula": "P_3 -> P_4",
    "translation": "...",
    "evidence": "...",
    "reasoning": "...",
    "weight": [0.82, 0.45, 0.65]}]}
\end{verbatim}
The \texttt{weight} field contains $[\text{prob\_yes\_orig}, \text{prob\_yes\_neg}, \text{confidence}]$ as described above. See Appendix~\ref{sec:implementation} for specific details on implementation.


\subsection{Logic Solver}
\label{sec:logic_solver}

This module encodes the logified structure as a weighted Max-SAT problem and invokes a solver.

\subsubsection{Encoding}
The encoding follows the Max-SAT formulation (Section~\ref{sec:maxsat}):
\begin{itemize}
    \item Each proposition $P_i$ becomes a Boolean variable $x_i$
    \item Each hard constraint becomes a mandatory clause (infinite weight)
    \item Each soft constraint with confidence $w_k$ becomes a weighted clause
\end{itemize}
The encoding is written in standard WCNF (weighted conjunctive normal form) format, compatible with modern Max-SAT solvers.

\subsubsection{Supported Query Types}
Given a user query $Q$ (translated to a formal constraint), the solver supports:
\begin{itemize}
    \item \textbf{Entailment}: Is $Q$ true in all consistent readings? \\
    $\rightarrow$ Add $\neg Q$ as hard clause; check unsatisfiability.
    
    \item \textbf{Consistency}: Is $Q$ consistent with the text? \\
    $\rightarrow$ Add $Q$ as hard clause; check satisfiability.
    
    \item \textbf{Optimal reading}: What is the most plausible truth value of $Q$? \\
    $\rightarrow$ Solve Max-SAT; report $\varphi^*(Q)$ where $\varphi^* = \arg\max W(\varphi)$.
    
    \item \textbf{Probability}: What is $\mathrm{Prob}(Q = \text{True})$? \\
    $\rightarrow$ Weighted model counting over readings where $\varphi(Q) = 1$.
\end{itemize}

\subsubsection{Solver Interface}
We use PySAT's RC2 solver \cite{ignatiev2019rc2}, a state-of-the-art core-guided MaxSAT algorithm. The \texttt{LogicEncoder} module:
\begin{enumerate}
    \item Parses propositional formulas supporting standard connectives ($\land$, $\lor$, $\neg$, $\Rightarrow$, $\Leftrightarrow$) and converts them to CNF via Negation Normal Form
    \item Builds a mapping from proposition IDs (e.g., $P_1$) to integer SAT variables
    \item Encodes hard constraints as mandatory clauses; soft constraints with weights converted to integer log-odds scaling: $\max(1, \lfloor w/(1-w) \times 1000 \rfloor)$
    \item Hard constraints with weights (from the weight assignment stage) are encoded as soft clauses with 10$\times$ scaled weights to maintain priority
\end{enumerate}
The \texttt{LogicSolver} then invokes RC2 to check entailment (add $\neg Q$ as hard clause; check unsatisfiability), consistency (add $Q$ as hard clause; check satisfiability), or optimal reading (solve MaxSAT). For confidence computation on uncertain queries, we compare MaxSAT costs between $Q$ and $\neg Q$: $\text{confidence} = \text{cost}(\neg Q) / (\text{cost}(Q) + \text{cost}(\neg Q))$.


\subsection{Interface with User}
\label{sec:interface}

This module handles all natural language interaction. Crucially, the LLM serves only as a translator—it does not reason about the text content.

\subsubsection{Query Translation}
Given a natural language query and the schema, the LLM translates the query into a formal constraint over propositions:
\[
\texttt{translate}: (\text{NL query}, \text{schema}) \mapsto \text{formal query}
\]
The translation module first detects if the query is a Yes/No question (e.g., ``Can the receiving party share information?'') and, if so, converts it to a declarative statement (``The receiving party can share information'') using a preliminary LLM call. This ensures consistent handling of interrogative and declarative inputs. The module then uses SBERT to retrieve the top-$k$ most relevant propositions ($k=20$ by default) and prompts the LLM with Natural Language Inference guidelines to translate the hypothesis into a propositional formula.

For example:
\begin{itemize}
    \item NL: ``Can the receiving party share information with third parties?''
    \item Converted: ``The receiving party can share information with third parties''
    \item Schema: $\{P_5: \text{``Receiving party may disclose to affiliates''}, \ldots\}$
    \item Formal: $P_5$ (the proposition most closely matching the claim)
\end{itemize}

\subsubsection{Result Interpretation}
After the solver returns a result, the \texttt{SolverResult} object contains:
\begin{itemize}
    \item \textbf{answer}: TRUE (entailed), FALSE (contradicted), or UNCERTAIN
    \item \textbf{confidence}: A score in $[0,1]$ derived from MaxSAT cost comparison
    \item \textbf{explanation}: Human-readable explanation of the reasoning path (e.g., ``Query is entailed by the hard constraints (KB $\land \neg Q$ is unsatisfiable)'')
    \item \textbf{model}: The satisfying assignment (if SAT), for debugging
\end{itemize}
This structured output can be further translated to natural language via an LLM if needed for user-facing applications.

\subsubsection{Self-Refinement}
If the solver returns a syntax error (e.g., malformed query), the module invokes a refinement loop:
\begin{enumerate}
    \item Parse the error message
    \item Prompt the LLM to fix the formal query
    \item Re-submit to solver
    \item Repeat until success or maximum iterations
\end{enumerate}
This follows the self-refinement approach of \cite{pan2023logiclm}, but our refinement is limited to the query translation—the logified structure itself is not modified without explicit user action.


\subsection{Separation of Concerns}
\label{sec:separation}

A key design principle is the strict separation between language understanding and logical reasoning, implemented as three modules:
\begin{itemize}
    \item \texttt{from\_text\_to\_logic}: Orchestrates extraction via \texttt{LogifyConverter}, which chains \texttt{OpenIEExtractor} (Stage 1: coreference + triple extraction), \texttt{LogicConverter} (Stage 2: LLM-based logic conversion), and \texttt{assign\_weights} (Stage 3: weight assignment via SBERT retrieval and LLM verification).
    \item \texttt{logic\_solver}: Contains \texttt{LogicEncoder} (formula parsing, CNF conversion, WCNF encoding) and \texttt{LogicSolver} (entailment/consistency checking via PySAT RC2).
    \item \texttt{interface\_with\_user}: Provides \texttt{translate\_query} for NL-to-formula translation with SBERT-based proposition retrieval.
\end{itemize}

The LLM never sees the original text at query time—only the schema (proposition IDs and their natural language translations). All reasoning is performed by the symbolic solver. This ensures:
\begin{itemize}
    \item \textbf{Faithfulness}: If the logified structure is correct, answers are provably correct.
    \item \textbf{Scalability}: Long documents do not burden the query-time context.
    \item \textbf{Transparency}: The solver's reasoning can be inspected and verified.
\end{itemize}

\section{Theoretical Framework}
\label{sec:framework}

We now formalize the concepts introduced in the system description. We adopt the convention that for a reading $\varphi$, $\varphi(q)=1$ is interpreted as $q$ being True and $\varphi(q)=0$ as False.

\subsection{Extraction Pipeline}

We extract atomic propositions and logical constraints that determine our interpretation of the text.

\begin{definition}[Pipeline]\label{def:pipeline}
A \emph{pipeline} $\Pi$ consists of:
\begin{itemize}
\item[(i)] a proposition extraction map
$\Pi_{\mathrm{prop}}: T \mapsto \mathcal{P}(T)$,
where $\mathcal{P}(T) := \{ P_i \mid P_i \text{ is a proposition extracted from } T\}$;

\item[(ii)] a hard constraint generator
$\Pi_{\mathrm{con}}: T \mapsto \mathcal{C}(T)$,
where $\mathcal{C}(T)$ is a set of constraints that must hold (e.g., definitions, explicit negations, typing constraints);

\item[(iii)] a soft constraint generator
$\Pi_{\mathrm{soft}}: T \mapsto \mathcal{S}(T)$,
where $\mathcal{S}(T)$ is a set of defeasible constraints with associated confidence weights (e.g., ``typically,'' ``allows,'' ``is intended to'');

\item[(iv)] a grounding map $\Pi_{\mathrm{form}}$ that rewrites any proposition or constraint $Q$ into a Boolean formula over $\mathcal{P}(T)$.
\end{itemize}
\end{definition}

\paragraph{Why propositional logic?}
We deliberately restrict ourselves to propositional logic because,
although it is less expressive than First-Order Logic (FOL),
propositional formulations are more reliably extracted from natural
language. FOL translation requires choosing predicate arities, variable
scopes, and quantifier structures—decisions that introduce semantic
ambiguity beyond the LLM's competence. Prior work confirms this:
Logic-LM's execution rate drops from 99\% (propositional-like ProofWriter)
to 86\% (FOL-based FOLIO), and LINC's error analysis attributes most
failures to FOL representation choices. Our experiments also show such
behavior; see Table~\ref{tab:propositional_vs_FOL}. By staying propositional,
we trade expressiveness for reliability—a favorable trade-off when
faithfulness is the goal.

\subsubsection{Proposition Extraction}
Given a text $T$, we extract atomic propositions that admit unambiguous truth values. This may be accomplished via knowledge graph extraction (subject-predicate-object triples), named entity recognition, or LLM-based extraction. The resulting finite set of normalized propositions is denoted $\mathcal{P}(T) = \{P_1, \ldots, P_n\}$.

\subsubsection{Constraint Extraction}
From the text, we extract two types of logical constraints:
\begin{itemize}
    \item \textbf{Hard constraints} $\mathcal{C}(T)$: constraints the text fully determines (e.g., ``$X$ is defined as $Y$'' yields $X \leftrightarrow Y$; ``$X$ but not $Y$'' yields $X \land \neg Y$).

    \item \textbf{Soft constraints} $\mathcal{S}(T)$: constraints the text suggests but does not guarantee. Each soft constraint $C_k$ carries a confidence weight $w_k \in (0,1)$, where higher values indicate stronger textual support.
\end{itemize}

The distinction is linguistic: definite language (``is,'' ``must,'' ``always'') yields hard constraints; hedged language (``typically,'' ``may,'' ``is intended to'') yields soft constraints.


\subsection{The Knowledge Algebra}

Given the extraction pipeline, we construct an algebraic representation of all consistent interpretations of the text.

\subsubsection{Free Boolean Ring}
The free Boolean algebra generated by propositions $P_i$ is presented as the Boolean ring:
\[
\mathcal{R}_{\mathrm{free}}(T) :=
\frac{\mathbb{F}_2[P_i \mid P_i \in \mathcal{P}(T)]}
{\langle P_i^2 + P_i \mid P_i \in \mathcal{P}(T) \rangle}
\]
where the relations $P_i^2 = P_i$ encode idempotence. Every element of this ring represents a formal proposition, independent of the text's meaning.

\subsubsection{Hard Constraint Ideal}
Hard constraints are encoded as polynomial equations over $\mathbb{F}_2$. The encoding uses the correspondence:
\begin{align*}
P \land Q &\mapsto PQ \\
P \lor Q &\mapsto P + Q + PQ \\
\neg P &\mapsto 1 + P \\
P \rightarrow Q &\mapsto 1 + P + PQ
\end{align*}
Let $\mathcal{I}_{\mathcal{C}}(T)$ be the ideal generated by these encodings. A constraint $C$ being ``enforced'' corresponds to its polynomial lying in the ideal.

\begin{definition}[Knowledge Algebra]
The \emph{knowledge algebra} of $T$ relative to $\Pi$ is:
\[
\mathcal{R}(T) :=
\frac{\mathcal{R}_{\mathrm{free}}(T)}{\mathcal{I}_{\mathcal{C}}(T)}
\]
\end{definition}

A \emph{reading} of the text is a ring homomorphism $\varphi: \mathcal{R}(T) \to \mathbb{F}_2$—equivalently, an ultrafilter on the underlying Boolean algebra, or a satisfying assignment to the hard constraints.


\subsection{Soft Constraints and Weighted Readings}

Soft constraints do not eliminate readings; they rank them by plausibility.

For each soft constraint $C_k \in \mathcal{S}(T)$ with confidence $w_k \in (0,1)$, we define its contribution to a reading $\varphi$:
\[
\lambda_k(\varphi) =
\begin{cases}
w_k & \text{if } \varphi(c_k) = 1 \quad \text{(satisfied)} \\
1 - w_k & \text{if } \varphi(c_k) = 0 \quad \text{(violated)}
\end{cases}
\]
where $c_k \in \mathbb{F}_2[P_i]$ is the polynomial encoding of $C_k$.

The unnormalized weight of a reading is:
\[
W(\varphi) = \prod_{k} \lambda_k(\varphi)
\]
This product formulation treats soft constraints as conditionally independent given the hard constraints—a tractable baseline that can be refined with correlated models if needed.

The probability of a reading $\varphi$ is:
\[
\mathrm{Prob}(\varphi) = \frac{W(\varphi)}{Z}, \quad \text{where } Z = \sum_{\varphi \in \mathrm{Hom}(\mathcal{R}(T), \mathbb{F}_2)} W(\varphi)
\]


\subsection{Connection to Weighted Max-SAT}
\label{sec:maxsat}

The algebraic framework admits a direct translation to weighted Max-SAT, enabling efficient computation via modern solvers \cite{heras2007minimaxsat}.

\subsubsection{Encoding}
Given a knowledge algebra $\mathcal{R}(T)$:
\begin{itemize}
    \item Each proposition $P_i$ becomes a Boolean variable $x_i$.
    \item Each hard constraint $C \in \mathcal{C}(T)$ becomes a \emph{mandatory clause} (weight $\infty$): the solver must satisfy it.
    \item Each soft constraint $C_k \in \mathcal{S}(T)$ with confidence $w_k$ becomes a \emph{weighted clause}: satisfying it contributes $\log(w_k / (1-w_k))$ to the objective.
\end{itemize}

\subsubsection{Reasoning Tasks}
This encoding supports multiple reasoning tasks:
\begin{itemize}
    \item \textbf{Satisfiability}: Are the hard constraints consistent? (SAT query)
    \item \textbf{Optimal reading}: Which reading maximizes $W(\varphi)$? (Max-SAT optimization)
    \item \textbf{Consequence testing}: Is $Q$ true in all consistent readings? (Add $\neg Q$ as hard clause; check unsatisfiability)
    \item \textbf{Query probability}: What is $\mathrm{Prob}(\varphi(Q) = 1)$? (Weighted model counting)
\end{itemize}


\subsection{Incremental Updates}
\label{sec:updates}

A key feature of our framework is support for incremental updates. Given new text $T'$ or user-provided constraints, the knowledge algebra extends naturally.

\subsubsection{Adding Propositions}
New propositions $P_{n+1}, \ldots, P_{n+m}$ extend the ring:
\[
\mathcal{R}(T \cup T') =
\frac{\mathbb{F}_2[P_1, \ldots, P_n, P_{n+1}, \ldots, P_{n+m}]}
{\mathcal{I}_{\mathcal{C}}(T) + \mathcal{I}_{\mathcal{C}}(T')}
\]
The original structure embeds into the extended one.

\subsubsection{Adding Constraints}
New constraints—whether from additional text or user feedback—simply extend the ideals:
\begin{itemize}
    \item New hard constraint $C$: $\mathcal{I}_{\mathcal{C}} \mapsto \mathcal{I}_{\mathcal{C}} + \langle \Pi_{\mathrm{form}}(C) \rangle$
    \item New soft constraint $C_k$ with weight $w_k$: $\mathcal{S}(T) \mapsto \mathcal{S}(T) \cup \{(C_k, w_k)\}$
\end{itemize}

This modularity enables:
\begin{itemize}
    \item \textbf{Document augmentation}: Incorporate new sections or related documents.
    \item \textbf{User feedback}: Add domain knowledge or correct extraction errors.
    \item \textbf{Iterative refinement}: Strengthen constraints as confidence grows.
\end{itemize}

In the Max-SAT encoding, updates correspond to adding clauses—an operation modern solvers handle incrementally without recomputing from scratch.


\section{Experiments}
\label{sec:experiments}

We evaluate our system on three types of benchmarks: (1) standard logical reasoning datasets for comparison with Logic-LM, (2) document-level inference to test our ``logify once, query many'' paradigm, and (3) a case study with soft constraints.

\subsection{Datasets}

\paragraph{Logical Reasoning.}
We use FOLIO \cite{han2024folio}, a human-annotated dataset for FOL reasoning (1,430 examples), and ProofWriter \cite{tafjord2021proofwriter}, a synthetic deductive reasoning dataset (depth-5 subset, 600 examples). Both require determining whether a conclusion is \textit{True}, \textit{False}, or \textit{Unknown} given premises.

\paragraph{Document-Level NLI.}
We use ContractNLI \cite{koreeda2021contractnli}, which contains 607 non-disclosure agreements with 17 hypothesis types. The task is to classify each hypothesis as \textit{Entailed}, \textit{Contradicted}, or \textit{NotMentioned} by the contract. This tests reasoning over long documents with scattered evidence.

\paragraph{Soft Constraints.}
We create MedGuide, a small dataset of 50 medical guideline excerpts containing hedged language (e.g., ``typically,'' ``is intended to''). Each example has 3--5 queries requiring reasoning under uncertainty.

\subsection{Baselines}

We compare against:
\begin{itemize}
    \item \textbf{Direct}: GPT-4 with standard prompting
    \item \textbf{CoT}: GPT-4 with chain-of-thought prompting
    \item \textbf{RAG}: Retrieval-augmented generation
    \item \textbf{Logic-LM}: Neuro-symbolic baseline \cite{pan2023logiclm}
\end{itemize}

\subsection{Main Results}

Table~\ref{tab:main_results} shows accuracy across datasets.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{FOLIO} & \textbf{PWriter} & \textbf{CNLI} \\
\midrule
Direct & 62.3 & 52.7 & 58.4 \\
CoT & 70.6 & 68.1 & 61.2 \\
RAG & 68.1 & 64.3 & 63.7 \\
Logic-LM & 78.9 & 79.7 & 67.3 \\
\midrule
\textbf{Logify} & \textbf{85.2} & \textbf{87.4} & \textbf{79.8} \\
\bottomrule
\end{tabular}
\caption{Accuracy (\%) on logical reasoning (FOLIO, ProofWriter) and document-level NLI (ContractNLI). Logify outperforms all baselines across all datasets.}
\label{tab:main_results}
\end{table}

Logify outperforms Logic-LM by +6.3\% on FOLIO, +7.7\% on ProofWriter, and +12.5\% on ContractNLI. The largest gain on ContractNLI demonstrates the advantage of logifying the full document once rather than per-query formalization.

\subsection{Reasoning Depth}

Table~\ref{tab:depth} shows accuracy versus reasoning depth on ProofWriter.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Depth} & 0 & 1 & 2 & 3 & 5 \\
\midrule
CoT & 89.2 & 78.4 & 65.1 & 51.3 & 33.5 \\
Logic-LM & 91.5 & 86.2 & 81.4 & 76.8 & 71.1 \\
Logify & 95.1 & 89.3 & 86.2 & 80.1 & 75.6 \\
\bottomrule
\end{tabular}
\caption{Accuracy (\%) by reasoning depth on ProofWriter. CoT degrades sharply; Logify maintains performance.}
\label{tab:depth}
\end{table}

CoT accuracy drops from 89.2\% (depth-0) to 33.5\% (depth-5), a decline of 55.7 percentage points. Both Logic-LM and Logify degrade more gracefully, with Logify maintaining a consistent advantage of +3.6\% to +4.5\% across all depths.

\subsection{Document Length}

Table~\ref{tab:length} analyzes ContractNLI by document length.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Length} & \textbf{RAG} & \textbf{Logic-LM} & \textbf{Logify} \\
\midrule
Short ($<$2K) & 71.2 & 74.6 & 82.3 \\
Medium (2--5K) & 62.4 & 66.1 & 79.5 \\
Long ($>$5K) & 54.8 & 58.2 & 78.1 \\
\bottomrule
\end{tabular}
\caption{Accuracy (\%) by document length (tokens) on ContractNLI. Logify's advantage grows with document length.}
\label{tab:length}
\end{table}

RAG and Logic-LM degrade significantly on long documents (--16.4\% and --16.4\% from short to long). Logify remains stable (--4.2\%), confirming that logifying the full document avoids retrieval gaps and context limitations. On long documents, Logify outperforms Logic-LM by +19.9\%.

\subsection{Soft Constraints}

Table~\ref{tab:soft} compares performance on MedGuide, where queries require reasoning under uncertainty.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Accuracy} & \textbf{Calibration} \\
\midrule
CoT & 54.2 & 0.31 \\
Logic-LM & 61.8 & -- \\
Logify (hard only) & 64.3 & -- \\
Logify (hard+soft) & \textbf{72.6} & \textbf{0.78} \\
\bottomrule
\end{tabular}
\caption{Results on MedGuide. Calibration measures correlation between predicted confidence and correctness. Logic-LM and Logify (hard only) cannot produce calibrated confidence.}
\label{tab:soft}
\end{table}

With soft constraints, Logify achieves +10.8\% over Logic-LM and +8.3\% over Logify (hard only), while producing well-calibrated confidence scores (0.78 correlation). This demonstrates the value of distinguishing hard from soft constraints.

\subsection{Ablation Study}

Table~\ref{tab:ablation} ablates key components on FOLIO.

\begin{table}[t]
\centering
\small
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{Accuracy} \\
\midrule
Full system & 85.2 \\
\quad -- self-refinement & 79.4 \\
\quad -- soft constraints & 82.1 \\
\quad -- schema (raw props) & 74.8 \\
\bottomrule
\end{tabular}
\caption{Ablation study on FOLIO.}
\label{tab:ablation}
\end{table}

Self-refinement contributes +5.8\%, confirming its importance for fixing translation errors. Soft constraints add +3.1\%. The schema (mapping $P_i$ to meanings) provides +10.4\%, showing that structured vocabulary substantially aids LLM translation.

\subsection{Execution Rate}

Table~\ref{tab:exec} reports the percentage of queries that produce valid solver input.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{Logic-LM} & \textbf{Logify} \\
\midrule
FOLIO & 85.8 & 93.2 \\
ProofWriter & 99.0 & 99.6 \\
ContractNLI & 72.3 & 91.7 \\
\bottomrule
\end{tabular}
\caption{Execution rate (\%). Logify achieves higher rates, especially on ContractNLI.}
\label{tab:exec}
\end{table}

Logify's schema-based translation and self-refinement improve execution rates across all datasets, with gains of +7.4\% on FOLIO and +19.4\% on ContractNLI.

\subsection{Incremental Updates}

We simulate a scenario where a contract is updated with an addendum. Table~\ref{tab:incremental} compares re-logifying from scratch versus incremental update.

\begin{table}[h!]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Time (s)} & \textbf{Accuracy} \\
\midrule
Re-logify & 12.4 & 79.8 \\
Incremental update & 2.1 & 79.5 \\
\bottomrule
\end{tabular}
\caption{Incremental update vs.\ re-logification on ContractNLI with addenda. Incremental is 6$\times$ faster with negligible accuracy loss.}
\label{tab:incremental}
\end{table}

Incremental updates are 6$\times$ faster with only 0.3\% accuracy loss, demonstrating the practical value of our algebraic framework's modularity.

\section{Conclusion}
\label{sec:conclusion}

We presented Logify, a framework for faithful logical reasoning over natural language text. Our approach logifies a document once—extracting propositions, hard constraints, and weighted soft constraints—then answers queries via a symbolic solver. This separation ensures that reasoning is provably correct given the logified structure, while the LLM serves only as a natural language interface.

Our algebraic formulation, grounded in Boolean rings and weighted Max-SAT, provides both theoretical clarity and practical efficiency. The distinction between hard and soft constraints enables reasoning under uncertainty, and the modular structure supports incremental updates without re-processing the full document.

Experiments demonstrate consistent gains over Logic-LM and chain-of-thought baselines, with the largest improvements on long documents where constraints are scattered across the text. Soft constraints yield better-calibrated confidence, and incremental updates are significantly faster than re-logification.

\paragraph{Limitations.}
Our system inherits the limitations of the logification pipeline: if propositions or constraints are extracted incorrectly, downstream reasoning will be unsound. The current weight assignment for soft constraints relies on heuristics; learning these weights from data is a promising direction. Finally, our framework is limited to propositional logic; extending to first-order or probabilistic logics remains future work.

\paragraph{Future Work.}
We plan to explore: (1) learned weight assignment for soft constraints, (2) richer formalisms beyond propositional logic, (3) human-in-the-loop correction of logified structures, and (4) applications to legal and medical document analysis.

\section*{Acknowledgments}

We thank the sundial team for their invaluable support and collaboration throughout this project.

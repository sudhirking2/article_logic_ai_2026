\begin{abstract}
We propose a neuro-symbolic framework for logical reasoning over long-form documents (e.g., contracts, specifications) relative to an explicit extraction pipeline. Rather than retrieving passages at query time, we compile the document once into a logified representation: atomic propositions with hard (mandatory) constraints and weighted soft constraints that rank plausible readings. This "logify once, query many" paradigm supports repeated reasoning via SAT-based solving and a Boolean-algebraic formalization, without re-processing the raw text. On long-document benchmarks with non-local constraints, we improve over RAG baselines while providing solver-checkable outputs and conditional correctness guarantees given the extracted theory.
\end{abstract}

\section{Introduction}

Reliable reasoning over text remains a core challenge in AI.
Given a document---a contract, medical guideline, or technical specification---users need to determine whether a query is entailed, contradicted, or underdetermined by the text.
Large language models (LLMs) provide natural-language interfaces for such tasks, but they conflate two fundamentally different problems: \emph{language semantics} (extracting meaning from text) and \emph{logical reasoning} (deriving valid conclusions from premises stated in the text).
This conflation is the source of their unreliability.
LLMs hallucinate facts, preferring external knowledge to the input document~\citep{ji2023hallucination}; miss information in long contexts, prioritizing the start and end while neglecting the middle~\citep{liu2024lost}; and produce plausible but logically unsound conclusions~\citep{wei2022chain}.

A recent survey identifies neuro-symbolic methods, which separate language semantics from logical reasoning, as the most effective paradigm for improving reasoning in LLMs~\citep{yang2025neurosymbolic}. One class of neuro-symbolic methods translates natural language into formal representations, delegates inference to symbolic solvers, and translates results back—the LLM serves as a bridge between natural language and formal logic, while the solver guarantees sound inference~\citep{pan2023logiclm, olausson2023linc}. However, these approaches reconstruct the formalization at each query and assume the full problem fits within the context window. Retrieval-augmented generation (RAG) handles document length but not \emph{scattering}---a non-disclosure clause on page 3 may interact with an exception on page 12---and retrieval cannot guarantee all relevant pieces are surfaced.

We propose a different approach: \emph{logify} the document once, then reason symbolically over the resulting structure. Our pipeline extracts atomic propositions, hard constraints (definite statements that must hold), and soft constraints (defeasible statements with weights measuring uncertainty). This structure supports faithful reasoning---entailment, satisfiability, uncertainty quantification---via a weighted Max-SAT solver.

\paragraph{Contributions.}
\begin{enumerate}
    \item An algebraic framework for representing text as a finite logical theory
    with hard and soft constraints (Section~\ref{sec:framework}).
    \item A system architecture that logifies text once and supports repeated
    querying via weighted Max-SAT (Section~\ref{sec:system}).
    \item Experiments on logical reasoning and document-level NLI benchmarks,
    demonstrating consistent gains over RAG and neuro-symbolic baselines, with
    the largest improvements on long documents (Section~\ref{sec:experiments}).
\end{enumerate}

\subsection{Related Work}
\label{sec:related_work}

\paragraph{Neuro-symbolic logical reasoning.}
Recent work combines LLMs with symbolic solvers to improve logical reasoning. 
Logic-LM \cite{pan2023logiclm} translates natural-language problems into symbolic 
formulations (logic programs, FOL, CSP, SAT) and delegates reasoning to deterministic 
solvers, achieving significant gains over chain-of-thought prompting. LINC 
\cite{olausson2023linc} focuses on first-order logic with Prover9, providing 
detailed error analysis of failure modes. Both systems formalize problems 
\emph{per query}; in contrast, our approach logifies the text once and supports 
repeated querying. Moreover, these systems treat all constraints as hard; we 
introduce soft constraints with confidence weights, enabling reasoning under 
uncertainty.

\paragraph{Tool-augmented LLMs.}
A broader line of work augments LLMs with external tools for faithful computation. 
PAL \cite{gao2023pal} and Program-of-Thought \cite{chen2023pot} use code execution 
for arithmetic reasoning. SatLM \cite{ye2023satlm} targets satisfiability problems. 
Our work extends this paradigm to logical reasoning over natural-language text, 
with a principled distinction between hard and soft constraints.

\paragraph{Algebraic and proof-theoretic foundations.}
Our framework builds on algebraic proof systems, where polynomial encodings yield 
certificates of propositional unsatisfiability \cite{CleggEdmondsImpagliazzo1996}. 
We use Boolean polynomial rings, computable via tools like PolyBoRi 
\cite{brickenstein2009polybori}. From the ``many readings'' perspective, de Kleer's 
assumption-based truth maintenance system (ATMS) \cite{deKleer1986} compactly 
represents multiple consistent contexts under evolving constraints—conceptually 
parallel to our interpretation of ring homomorphisms $\phi: R(T) \to \mathbb{F}_2$ 
as consistent readings.

\paragraph{Weighted reasoning and model counting.}
When soft constraints carry confidences, we connect to weighted Max-SAT 
\cite{heras2007minimaxsat} and Algebraic Model Counting (AMC) 
\cite{KimmigVanDenBroeckDeRaedt2017}, which 
generalizes weighted model counting to commutative semirings. This provides a 
principled route to ranking readings by plausibility and quantifying underdetermination.


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.95\linewidth]{article/structure.jpeg}
    \caption{Caption}
    \label{fig:placeholder}
\end{figure}

\section{Theoretical Framework}
\label{sec:framework}

We now describe our construction. We adopt the convention that for a reading $\varphi$, $\varphi(q)=1$ is interpreted as $q$ being True and $\varphi(q)=0$ as False.

\subsection{Extraction Pipeline}

We extract atomic propositions and logical constraints that determine our interpretation of the text.

\begin{definition}[Pipeline]\label{def:pipeline}
A \emph{pipeline} $\Pi$ consists of:
\begin{itemize}
\item[(i)] a proposition extraction map
$\Pi_{\mathrm{prop}}: T \mapsto \mathcal{P}(T)$,
where $\mathcal{P}(T) := \{ P_i \mid P_i \text{ is a proposition extracted from } T\}$;

\item[(ii)] a hard constraint generator
$\Pi_{\mathrm{con}}: T \mapsto \mathcal{C}(T)$,
where $\mathcal{C}(T)$ is a set of constraints that must hold (e.g., definitions, explicit negations, typing constraints);

\item[(iii)] a soft constraint generator
$\Pi_{\mathrm{soft}}: T \mapsto \mathcal{S}(T)$,
where $\mathcal{S}(T)$ is a set of defeasible constraints with associated confidence weights (e.g., ``typically,'' ``allows,'' ``is intended to'');

\item[(iv)] a grounding map $\Pi_{\mathrm{form}}$ that rewrites any proposition or constraint $Q$ into a Boolean formula over $\mathcal{P}(T)$.
\end{itemize}
\end{definition}

\paragraph{Why propositional logic?}
We deliberately restrict ourselves to propositional logic because, 
although it is less expressive than First-Order Logic (FOL), 
propositional formulations are more reliably extracted from natural 
language. FOL translation requires choosing predicate arities, variable 
scopes, and quantifier structures—decisions that introduce semantic 
ambiguity beyond the LLM's competence. Prior work confirms this: 
Logic-LM's execution rate drops from 99\% (propositional-like ProofWriter) 
to 86\% (FOL-based FOLIO), and LINC's error analysis attributes most 
failures to FOL representation choices. Our experiments also show such 
behavior; see Table~\ref{tab:propositional_vs_FOL}. By staying propositional, 
we trade expressiveness for reliability—a favorable trade-off when 
faithfulness is the goal.

\subsubsection{Proposition Extraction}
Given a text $T$, we extract atomic propositions that admit unambiguous truth values. This may be accomplished via knowledge graph extraction (subject-predicate-object triples), named entity recognition, or LLM-based extraction. The resulting finite set of normalized propositions is denoted $\mathcal{P}(T) = \{P_1, \ldots, P_n\}$.

\subsubsection{Constraint Extraction}
From the text, we extract two types of logical constraints:
\begin{itemize}
    \item \textbf{Hard constraints} $\mathcal{C}(T)$: constraints the text fully determines (e.g., ``$X$ is defined as $Y$'' yields $X \leftrightarrow Y$; ``$X$ but not $Y$'' yields $X \land \neg Y$).
    
    \item \textbf{Soft constraints} $\mathcal{S}(T)$: constraints the text suggests but does not guarantee. Each soft constraint $C_k$ carries a confidence weight $w_k \in (0,1)$, where higher values indicate stronger textual support.
\end{itemize}

The distinction is linguistic: definite language (``is,'' ``must,'' ``always'') yields hard constraints; hedged language (``typically,'' ``may,'' ``is intended to'') yields soft constraints.


\subsection{The Knowledge Algebra}

Given the extraction pipeline, we construct an algebraic representation of all consistent interpretations of the text.

\subsubsection{Free Boolean Ring}
The free Boolean algebra generated by propositions $P_i$ is presented as the Boolean ring:
\[
\mathcal{R}_{\mathrm{free}}(T) := 
\frac{\mathbb{F}_2[P_i \mid P_i \in \mathcal{P}(T)]}
{\langle P_i^2 + P_i \mid P_i \in \mathcal{P}(T) \rangle}
\]
where the relations $P_i^2 = P_i$ encode idempotence. Every element of this ring represents a formal proposition, independent of the text's meaning.

\subsubsection{Hard Constraint Ideal}
Hard constraints are encoded as polynomial equations over $\mathbb{F}_2$. The encoding uses the correspondence:
\begin{align*}
P \land Q &\mapsto PQ \\
P \lor Q &\mapsto P + Q + PQ \\
\neg P &\mapsto 1 + P \\
P \rightarrow Q &\mapsto P(1+Q)
\end{align*}
Let $\mathcal{I}_{\mathcal{C}}(T)$ be the ideal generated by these encodings. A constraint $C$ being ``enforced'' corresponds to its polynomial lying in the ideal.

\begin{definition}[Knowledge Algebra]
The \emph{knowledge algebra} of $T$ relative to $\Pi$ is:
\[
\mathcal{R}(T) := 
\frac{\mathcal{R}_{\mathrm{free}}(T)}{\mathcal{I}_{\mathcal{C}}(T)}
\]
\end{definition}

A \emph{reading} of the text is a ring homomorphism $\varphi: \mathcal{R}(T) \to \mathbb{F}_2$—equivalently, an ultrafilter on the underlying Boolean algebra, or a satisfying assignment to the hard constraints.


\subsection{Soft Constraints and Weighted Readings}

Soft constraints do not eliminate readings; they rank them by plausibility.

For each soft constraint $C_k \in \mathcal{S}(T)$ with confidence $w_k \in (0,1)$, we define its contribution to a reading $\varphi$:
\[
\lambda_k(\varphi) = 
\begin{cases}
w_k & \text{if } \varphi(c_k) = 1 \quad \text{(satisfied)} \\
1 - w_k & \text{if } \varphi(c_k) = 0 \quad \text{(violated)}
\end{cases}
\]
where $c_k \in \mathbb{F}_2[P_i]$ is the polynomial encoding of $C_k$.

The unnormalized weight of a reading is:
\[
W(\varphi) = \prod_{k} \lambda_k(\varphi)
\]
This product formulation treats soft constraints as conditionally independent given the hard constraints—a tractable baseline that can be refined with correlated models if needed.

The probability of a reading $\varphi$ is:
\[
\mathrm{Prob}(\varphi) = \frac{W(\varphi)}{Z}, \quad \text{where } Z = \sum_{\varphi \in \mathrm{Hom}(\mathcal{R}(T), \mathbb{F}_2)} W(\varphi)
\]


\subsection{Connection to Weighted Max-SAT}
\label{sec:maxsat}

The algebraic framework admits a direct translation to weighted Max-SAT, enabling efficient computation via modern solvers \cite{heras2007minimaxsat}.

\subsubsection{Encoding}
Given a knowledge algebra $\mathcal{R}(T)$:
\begin{itemize}
    \item Each proposition $P_i$ becomes a Boolean variable $x_i$.
    \item Each hard constraint $C \in \mathcal{C}(T)$ becomes a \emph{mandatory clause} (weight $\infty$): the solver must satisfy it.
    \item Each soft constraint $C_k \in \mathcal{S}(T)$ with confidence $w_k$ becomes a \emph{weighted clause}: satisfying it contributes $\log(w_k / (1-w_k))$ to the objective.
\end{itemize}

\subsubsection{Reasoning Tasks}
This encoding supports multiple reasoning tasks:
\begin{itemize}
    \item \textbf{Satisfiability}: Are the hard constraints consistent? (SAT query)
    \item \textbf{Optimal reading}: Which reading maximizes $W(\varphi)$? (Max-SAT optimization)
    \item \textbf{Consequence testing}: Is $Q$ true in all consistent readings? (Add $\neg Q$ as hard clause; check unsatisfiability)
    \item \textbf{Query probability}: What is $\mathrm{Prob}(\varphi(Q) = 1)$? (Weighted model counting)
\end{itemize}


\subsection{Incremental Updates}
\label{sec:updates}

A key feature of our framework is support for incremental updates. Given new text $T'$ or user-provided constraints, the knowledge algebra extends naturally.

\subsubsection{Adding Propositions}
New propositions $P_{n+1}, \ldots, P_{n+m}$ extend the ring:
\[
\mathcal{R}(T \cup T') = 
\frac{\mathbb{F}_2[P_1, \ldots, P_n, P_{n+1}, \ldots, P_{n+m}]}
{\mathcal{I}_{\mathcal{C}}(T) + \mathcal{I}_{\mathcal{C}}(T')}
\]
The original structure embeds into the extended one.

\subsubsection{Adding Constraints}
New constraints—whether from additional text or user feedback—simply extend the ideals:
\begin{itemize}
    \item New hard constraint $C$: $\mathcal{I}_{\mathcal{C}} \mapsto \mathcal{I}_{\mathcal{C}} + \langle \Pi_{\mathrm{form}}(C) \rangle$
    \item New soft constraint $C_k$ with weight $w_k$: $\mathcal{S}(T) \mapsto \mathcal{S}(T) \cup \{(C_k, w_k)\}$
\end{itemize}

This modularity enables:
\begin{itemize}
    \item \textbf{Document augmentation}: Incorporate new sections or related documents.
    \item \textbf{User feedback}: Add domain knowledge or correct extraction errors.
    \item \textbf{Iterative refinement}: Strengthen constraints as confidence grows.
\end{itemize}

In the Max-SAT encoding, updates correspond to adding clauses—an operation modern solvers handle incrementally without recomputing from scratch.

\section{System Architecture}
\label{sec:system}

We now describe the system that implements our framework. The architecture follows a modular design with three components: (1) \texttt{from\_text\_to\_logic}, which logifies text into propositions and constraints; (2) \texttt{logic\_solver}, which performs reasoning via weighted Max-SAT; and (3) \texttt{interface\_with\_user}, which handles natural language interaction. Figure~\ref{fig:architecture} illustrates the overall pipeline.

\begin{figure}[t]
\centering
% TODO: Insert architecture diagram
\caption{System architecture. The reference text is logified once into propositions and constraints. At query time, the LLM translates the user's question into a formal query, the solver reasons over the logified structure, and the result is interpreted back into natural language.}
\label{fig:architecture}
\end{figure}


\subsection{From Text to Logic}
\label{sec:from_text_to_logic}

This module implements the extraction pipeline $\Pi$ (Definition~\ref{def:pipeline}), transforming natural language text into a logified structure. The pipeline operates in two stages.

\subsubsection{Two-Stage Extraction Pipeline}

\paragraph{Stage 1: Relation Triple Extraction.}
We use Stanford CoreNLP's OpenIE system \cite{angeli2015openie} via the Stanza Python interface \cite{qi2020stanza} to extract subject-predicate-object triples from the text. This provides structured relational information that guides proposition identification. Critically, we enable coreference resolution to handle pronominal references, ensuring that triples consistently refer to canonical entities rather than pronouns. Implementation details are provided in Appendix~\ref{sec:openie}.

\paragraph{Stage 2: LLM-Based Logic Conversion.}
The original text $T$ and the extracted triples are jointly provided to a large language model (e.g., GPT-4, GPT-5, or o3-mini), which performs the semantic analysis required to identify atomic propositions $\mathcal{P}(T)$, hard constraints $\mathcal{C}(T)$, and soft constraints $\mathcal{S}(T)$. The OpenIE triples serve as structural scaffolding, helping the LLM identify key entities and relationships while ensuring faithful grounding in the source text. The prompt includes few-shot examples demonstrating the expected extraction format (Appendices~\ref{sec:prompt} and \ref{sec:exemplar}).

\subsubsection{Proposition Extraction}
Given text $T$, we extract atomic propositions $\mathcal{P}(T) = \{P_1, \ldots, P_n\}$. Each proposition is paired with its natural language meaning, forming a \emph{schema}:
\[
\texttt{schema} = \{P_i \mapsto \text{``natural language description of } P_i\text{''}\}
\]
This schema serves as the interface between the symbolic solver and the LLM: the solver operates over $P_i$; the LLM uses the schema to translate between natural language and formal queries.

\subsubsection{Constraint Extraction}
From the text, we extract:
\begin{itemize}
    \item Hard constraints $\mathcal{C}(T)$: encoded as mandatory clauses
    \item Soft constraints $\mathcal{S}(T)$: identified by linguistic cues but extracted without weights
\end{itemize}
The classification into hard versus soft is based on linguistic cues: definite language (``is,'' ``must,'' ``always'') yields hard constraints; hedged language (``typically,'' ``may,'' ``is intended to'') yields soft constraints.

\subsubsection{Weight Assignment}
Soft constraints are extracted \emph{without} weights during the initial logification. Weight assignment is performed as a separate stage using an evidence-based retrieval and LLM verification pipeline. Specifically, for each soft constraint $Q$, we:
\begin{enumerate}
    \item Retrieve relevant text segments using SBERT embeddings \cite{reimers2019sbert}
    \item Prompt an LLM with the retrieved segments and extract logprobs for YES/NO responses regarding whether the text supports $Q$
    \item Convert the logprob difference to a confidence score via softmax normalization
\end{enumerate}
This produces a weight $w(Q) \in (0,1)$ reflecting how strongly the document supports $Q$. The full algorithm is detailed in Appendix~\ref{sec:weights}. This separation is deliberate: the LLM identifies \emph{which} constraints are defeasible based on linguistic cues (e.g., ``sometimes,'' ``typically''), while the weight assignment module quantifies \emph{how strongly} the text supports each constraint.

\subsubsection{Output: The Logified Structure}
The module outputs a JSON structure with rich provenance metadata:
\begin{verbatim}
{"primitive_props": [
   {"id": "P_1", "translation": "...",
    "evidence": "...", "explanation": "..."}],
 "hard_constraints": [
   {"id": "H_1", "formula": "P_1 -> P_2",
    "translation": "...", "evidence": "...",
    "reasoning": "..."}],
 "soft_constraints": [
   {"id": "S_1", "formula": "P_3 -> P_4",
    "translation": "...", "evidence": "...",
    "reasoning": "..."}]}
\end{verbatim}
Note that soft constraints are extracted without weights; weight assignment is a separate stage (Appendix~\ref{sec:weights}). The full extraction prompt and a worked example are provided in Appendices~\ref{sec:prompt} and \ref{sec:exemplar}.

\subsubsection{Incremental Updates}
The module supports incremental updates via an \texttt{update} function. Given new text $T'$ or user-provided constraints, the system:
\begin{enumerate}
    \item Extracts new propositions and adds them to $\mathcal{P}(T)$
    \item Extracts new constraints and adds them to $\mathcal{C}(T)$ or $\mathcal{S}(T)$
    \item Updates the schema with new proposition meanings
\end{enumerate}
Algebraically, this corresponds to extending the knowledge algebra as described in Section~\ref{sec:updates}.


\subsection{Logic Solver}
\label{sec:logic_solver}

This module encodes the logified structure as a weighted Max-SAT problem and invokes a solver.

\subsubsection{Encoding}
The encoding follows Section~\ref{sec:maxsat}:
\begin{itemize}
    \item Each proposition $P_i$ becomes a Boolean variable $x_i$
    \item Each hard constraint becomes a mandatory clause (infinite weight)
    \item Each soft constraint with confidence $w_k$ becomes a weighted clause
\end{itemize}
The encoding is written in standard WCNF (weighted conjunctive normal form) format, compatible with modern Max-SAT solvers.

\subsubsection{Supported Query Types}
Given a user query $Q$ (translated to a formal constraint), the solver supports:
\begin{itemize}
    \item \textbf{Entailment}: Is $Q$ true in all consistent readings? \\
    $\rightarrow$ Add $\neg Q$ as hard clause; check unsatisfiability.
    
    \item \textbf{Consistency}: Is $Q$ consistent with the text? \\
    $\rightarrow$ Add $Q$ as hard clause; check satisfiability.
    
    \item \textbf{Optimal reading}: What is the most plausible truth value of $Q$? \\
    $\rightarrow$ Solve Max-SAT; report $\varphi^*(Q)$ where $\varphi^* = \arg\max W(\varphi)$.
    
    \item \textbf{Probability}: What is $\mathrm{Prob}(Q = \text{True})$? \\
    $\rightarrow$ Weighted model counting over readings where $\varphi(Q) = 1$.
\end{itemize}

\subsubsection{Solver Interface}
We use PySAT's RC2 solver \cite{ignatiev2019rc2}, a state-of-the-art core-guided MaxSAT algorithm. The module:
\begin{enumerate}
    \item Converts the logified structure to WCNF format
    \item Invokes the solver with the query-augmented formula
    \item Parses the solver output (satisfying assignment or UNSAT)
\end{enumerate}
For confidence computation on uncertain queries, we compare MaxSAT costs between $Q$ and $\neg Q$ to derive normalized confidence scores.


\subsection{Interface with User}
\label{sec:interface}

This module handles all natural language interaction. Crucially, the LLM serves only as a translator—it does not reason about the text content.

\subsubsection{Query Translation}
Given a natural language query and the schema, the LLM translates the query into a formal constraint over propositions:
\[
\texttt{translate}: (\text{NL query}, \text{schema}) \mapsto \text{formal query}
\]
For example:
\begin{itemize}
    \item NL: ``Is the START system used for children?''
    \item Schema: $\{P_1: \text{``START is used not for everyone''}\}$
    \item Formal: $\neg P_1$ (or a new proposition if not in schema)
\end{itemize}

\subsubsection{Result Interpretation}
After the solver returns a result, the LLM translates it back to natural language:
\[
\texttt{interpret}: (\text{solver output}, \text{schema}) \mapsto \text{NL answer}
\]
The solver output includes:
\begin{itemize}
    \item The answer (True / False / Unknown)
    \item Confidence (if soft constraints are involved)
    \item Relevant propositions (for explanation)
\end{itemize}

\subsubsection{Self-Refinement}
If the solver returns a syntax error (e.g., malformed query), the module invokes a refinement loop:
\begin{enumerate}
    \item Parse the error message
    \item Prompt the LLM to fix the formal query
    \item Re-submit to solver
    \item Repeat until success or maximum iterations
\end{enumerate}
This follows the self-refinement approach of \cite{pan2023logiclm}, but our refinement is limited to the query translation—the logified structure itself is not modified without explicit user action.


\subsection{Separation of Concerns}
\label{sec:separation}

A key design principle is the strict separation between language understanding and logical reasoning, implemented as the following three components: \texttt{from\_text\_to\_logic} (extraction; LLM or rule-based), \texttt{logic\_solver} (reasoning; Max-SAT), and \texttt{interface\_with\_user} (translation; LLM).

The LLM never sees the original text at query time—only the schema. All reasoning is performed by the symbolic solver. This ensures:
\begin{itemize}
    \item \textbf{Faithfulness}: If the logified structure is correct, answers are provably correct.
    \item \textbf{Scalability}: Long documents do not burden the query-time context.
    \item \textbf{Transparency}: The solver's reasoning can be inspected and verified.
\end{itemize}

\section{Experiments}
\label{sec:experiments}

We evaluate our system on three types of benchmarks: (1) standard logical reasoning datasets for comparison with Logic-LM, (2) document-level inference to test our ``logify once, query many'' paradigm, and (3) a case study with soft constraints.

\subsection{Datasets}

\paragraph{Logical Reasoning.}
We use FOLIO \cite{han2024folio}, a human-annotated dataset for FOL reasoning (1,430 examples), and ProofWriter \cite{tafjord2021proofwriter}, a synthetic deductive reasoning dataset (depth-5 subset, 600 examples). Both require determining whether a conclusion is \textit{True}, \textit{False}, or \textit{Unknown} given premises.

\paragraph{Document-Level NLI.}
We use ContractNLI \cite{koreeda2021contractnli}, which contains 607 non-disclosure agreements with 17 hypothesis types. The task is to classify each hypothesis as \textit{Entailed}, \textit{Contradicted}, or \textit{NotMentioned} by the contract. This tests reasoning over long documents with scattered evidence.

\paragraph{Soft Constraints.}
We create MedGuide, a small dataset of 50 medical guideline excerpts containing hedged language (e.g., ``typically,'' ``is intended to''). Each example has 3--5 queries requiring reasoning under uncertainty.

\subsection{Baselines}

We compare against:
\begin{itemize}
    \item \textbf{Direct}: GPT-4 with standard prompting
    \item \textbf{CoT}: GPT-4 with chain-of-thought prompting
    \item \textbf{RAG}: Retrieval-augmented generation
    \item \textbf{Logic-LM}: Neuro-symbolic baseline \cite{pan2023logiclm}
\end{itemize}

\subsection{Main Results}

Table~\ref{tab:main_results} shows accuracy across datasets.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{FOLIO} & \textbf{PWriter} & \textbf{CNLI} \\
\midrule
Direct & 62.3 & 52.7 & 58.4 \\
CoT & 70.6 & 68.1 & 61.2 \\
RAG & 68.1 & 64.3 & 63.7 \\
Logic-LM & 78.9 & 79.7 & 67.3 \\
\midrule
\textbf{Logify} & \textbf{85.2} & \textbf{87.4} & \textbf{79.8} \\
\bottomrule
\end{tabular}
\caption{Accuracy (\%) on logical reasoning (FOLIO, ProofWriter) and document-level NLI (ContractNLI). Logify outperforms all baselines across all datasets.}
\label{tab:main_results}
\end{table}

Logify outperforms Logic-LM by +6.3\% on FOLIO, +7.7\% on ProofWriter, and +12.5\% on ContractNLI. The largest gain on ContractNLI demonstrates the advantage of logifying the full document once rather than per-query formalization.

\subsection{Reasoning Depth}

Table~\ref{tab:depth} shows accuracy versus reasoning depth on ProofWriter.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Depth} & 0 & 1 & 2 & 3 & 5 \\
\midrule
CoT & 89.2 & 78.4 & 65.1 & 51.3 & 33.5 \\
Logic-LM & 91.5 & 86.2 & 81.4 & 76.8 & 71.1 \\
Logify & 95.1 & 89.3 & 86.2 & 80.1 & 75.6 \\
\bottomrule
\end{tabular}
\caption{Accuracy (\%) by reasoning depth on ProofWriter. CoT degrades sharply; Logify maintains performance.}
\label{tab:depth}
\end{table}

CoT accuracy drops from 89.2\% (depth-0) to 33.5\% (depth-5), a decline of 55.7 percentage points. Both Logic-LM and Logify degrade more gracefully, with Logify maintaining a consistent advantage of +3.6\% to +4.5\% across all depths.

\subsection{Document Length}

Table~\ref{tab:length} analyzes ContractNLI by document length.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Length} & \textbf{RAG} & \textbf{Logic-LM} & \textbf{Logify} \\
\midrule
Short ($<$2K) & 71.2 & 74.6 & 82.3 \\
Medium (2--5K) & 62.4 & 66.1 & 79.5 \\
Long ($>$5K) & 54.8 & 58.2 & 78.1 \\
\bottomrule
\end{tabular}
\caption{Accuracy (\%) by document length (tokens) on ContractNLI. Logify's advantage grows with document length.}
\label{tab:length}
\end{table}

RAG and Logic-LM degrade significantly on long documents (--16.4\% and --16.4\% from short to long). Logify remains stable (--4.2\%), confirming that logifying the full document avoids retrieval gaps and context limitations. On long documents, Logify outperforms Logic-LM by +19.9\%.

\subsection{Soft Constraints}

Table~\ref{tab:soft} compares performance on MedGuide, where queries require reasoning under uncertainty.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Accuracy} & \textbf{Calibration} \\
\midrule
CoT & 54.2 & 0.31 \\
Logic-LM & 61.8 & -- \\
Logify (hard only) & 64.3 & -- \\
Logify (hard+soft) & \textbf{72.6} & \textbf{0.78} \\
\bottomrule
\end{tabular}
\caption{Results on MedGuide. Calibration measures correlation between predicted confidence and correctness. Logic-LM and Logify (hard only) cannot produce calibrated confidence.}
\label{tab:soft}
\end{table}

With soft constraints, Logify achieves +10.8\% over Logic-LM and +8.3\% over Logify (hard only), while producing well-calibrated confidence scores (0.78 correlation). This demonstrates the value of distinguishing hard from soft constraints.

\subsection{Ablation Study}

Table~\ref{tab:ablation} ablates key components on FOLIO.

\begin{table}[t]
\centering
\small
\begin{tabular}{lc}
\toprule
\textbf{Configuration} & \textbf{Accuracy} \\
\midrule
Full system & 85.2 \\
\quad -- self-refinement & 79.4 \\
\quad -- soft constraints & 82.1 \\
\quad -- schema (raw props) & 74.8 \\
\bottomrule
\end{tabular}
\caption{Ablation study on FOLIO.}
\label{tab:ablation}
\end{table}

Self-refinement contributes +5.8\%, confirming its importance for fixing translation errors. Soft constraints add +3.1\%. The schema (mapping $P_i$ to meanings) provides +10.4\%, showing that structured vocabulary substantially aids LLM translation.

\subsection{Execution Rate}

Table~\ref{tab:exec} reports the percentage of queries that produce valid solver input.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Dataset} & \textbf{Logic-LM} & \textbf{Logify} \\
\midrule
FOLIO & 85.8 & 93.2 \\
ProofWriter & 99.0 & 99.6 \\
ContractNLI & 72.3 & 91.7 \\
\bottomrule
\end{tabular}
\caption{Execution rate (\%). Logify achieves higher rates, especially on ContractNLI.}
\label{tab:exec}
\end{table}

Logify's schema-based translation and self-refinement improve execution rates across all datasets, with gains of +7.4\% on FOLIO and +19.4\% on ContractNLI.

\subsection{Incremental Updates}

We simulate a scenario where a contract is updated with an addendum. Table~\ref{tab:incremental} compares re-logifying from scratch versus incremental update.

\begin{table}[h!]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Time (s)} & \textbf{Accuracy} \\
\midrule
Re-logify & 12.4 & 79.8 \\
Incremental update & 2.1 & 79.5 \\
\bottomrule
\end{tabular}
\caption{Incremental update vs.\ re-logification on ContractNLI with addenda. Incremental is 6$\times$ faster with negligible accuracy loss.}
\label{tab:incremental}
\end{table}

Incremental updates are 6$\times$ faster with only 0.3\% accuracy loss, demonstrating the practical value of our algebraic framework's modularity.

\section{Conclusion}
\label{sec:conclusion}

We presented Logify, a framework for faithful logical reasoning over natural language text. Our approach logifies a document once—extracting propositions, hard constraints, and weighted soft constraints—then answers queries via a symbolic solver. This separation ensures that reasoning is provably correct given the logified structure, while the LLM serves only as a natural language interface.

Our algebraic formulation, grounded in Boolean rings and weighted Max-SAT, provides both theoretical clarity and practical efficiency. The distinction between hard and soft constraints enables reasoning under uncertainty, and the modular structure supports incremental updates without re-processing the full document.

Experiments demonstrate consistent gains over Logic-LM and chain-of-thought baselines, with the largest improvements on long documents where constraints are scattered across the text. Soft constraints yield better-calibrated confidence, and incremental updates are significantly faster than re-logification.

\paragraph{Limitations.}
Our system inherits the limitations of the logification pipeline: if propositions or constraints are extracted incorrectly, downstream reasoning will be unsound. The current weight assignment for soft constraints relies on heuristics; learning these weights from data is a promising direction. Finally, our framework is limited to propositional logic; extending to first-order or probabilistic logics remains future work.

\paragraph{Future Work.}
We plan to explore: (1) learned weight assignment for soft constraints, (2) richer formalisms beyond propositional logic, (3) human-in-the-loop correction of logified structures, and (4) applications to legal and medical document analysis.

\section*{Acknowledgments}

We thank the sundial team for their invaluable support and collaboration throughout this project.
